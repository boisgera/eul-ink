<html lang="en"><head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Sébastien Boisgérault, Mines ParisTech">
  <meta name="dcterms.date" content="2017-02-13">
  <title>Binary Coding</title>
  <style type="text/css">code{white-space: pre;}</style>
  
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
<style type="text/css">* {
  margin: 0;
  border: 0;
  font-size: 100%;
  font: inherit;
}
*, table {
  padding: 0;
}
*, html main {
  box-sizing: content-box;
}
*, nav#TOC .badge {
  vertical-align: baseline;
}
html, main > header .author, main > header .date, main > .header .author, main > .header .date, main > #header .author, main > #header .date, h2, .section-flag {
  line-height: 36px;
}
html, main > header .author, main > header .date, main > .header .author, main > .header .date, main > #header .author, main > #header .date, h2, h3, h4, h5, h6, code {
  font-size: 24px;
}
html {
  font-style: normal;
  font-family: Alegreya, serif;
  text-rendering: optimizeLegibility;
  text-align: left;
}
html, main > header .author, main > header .date, main > .header .author, main > .header .date, main > #header .author, main > #header .date, nav#TOC > ul ul li {
  font-weight: normal;
}
ol, ul {
  list-style: none;
}
blockquote {
  quotes: none;
  border-left-width: thick;
  border-left-style: solid;
  border-left-color: black;
}
blockquote, html main {
  padding: 36px;
}
blockquote, html p, html .p, html section, main > header h1, main > .header h1, main > #header h1, pre, figure, .table, nav#TOC > ul > * {
  margin-bottom: 36px;
}
blockquote:before, blockquote:after {
  content: none;
}
table {
  border-collapse: collapse;
  border-spacing: 1em 12px;
  border-top: medium solid black;
}
table, img {
  margin-left: auto;
  margin-right: auto;
}
table, thead {
  border-bottom: medium solid black;
}
html em, figcaption {
  font-style: italic;
}
html strong, main > header h1, main > .header h1, main > #header h1, h1, h2, h3, h4, h5, h6, nav#TOC > ul {
  font-weight: bold;
}
html p, html .p, figcaption {
  text-align: justify;
}
html p, html .p {
  hyphens: auto;
  -moz-hyphens: auto;
}
html main {
  max-width: 32em;
  margin: auto;
}
main > header, main > .header, main > #header, h1 {
  margin-top: 72px;
}
main > header, main > .header, main > #header {
  margin-bottom: 72px;
}
main > header h1, main > .header h1, main > #header h1 {
  font-size: 48px;
  line-height: 54px;
  margin-top: 0px;
}
main > header .author, main > header .date, main > .header .author, main > .header .date, main > #header .author, main > #header .date, h2 {
  margin-bottom: 18px;
}
main > header .date, main > .header .date, main > #header .date {
  font-family: "Alegreya SC", serif;
  float: none;
}
h1 {
  font-size: 34px;
  line-height: 45px;
  margin-bottom: 27px;
}
h3, h4, h5, h6, nav#TOC .badge {
  margin-right: 1em;
}
h3, h4, h5, h6 {
  display: inline;
}
a {
  cursor: pointer;
  outline: 0;
}
a, a:hover {
  text-decoration: none;
}
a:link, a:visited {
  color: black;
}
sup {
  vertical-align: super;
  line-height: 0;
}
li, nav#TOC > ul li {
  list-style-type: none;
}
li {
  list-style-image: none;
  list-style-position: outside;
  padding-left: 0.5em;
}
li, nav#TOC > ul ul li {
  margin-left: 36px;
}
ul li {
  list-style: disc;
}
ol li {
  list-style: decimal;
}
blockquote p:last-child {
  margin-bottom: 0px;
}
code {
  font-family: Inconsolata;
}
pre, .table, .MJXc-display {
  overflow-x: auto;
}
pre {
  background-color: #ebebeb;
  padding-left: 36px;
  padding-right: 36px;
  padding-top: 36px;
}
pre, nav#TOC > ul > li.top-li {
  padding-bottom: 36;
}
img {
  display: block;
  height: auto;
}
img, .table, .MJXc-display {
  width: 100%;
}
figure, nav#TOC .badge {
  text-align: center;
}
figcaption, nav#TOC .badge {
  display: inline-block;
}
.table, .MJXc-display {
  overflow-y: hidden;
}
td, th {
  padding: 6px 0.5em;
}
nav#TOC > ul, nav#TOC .badge {
  position: relative;
}
nav#TOC > ul li {
  margin-left: 0;
  padding-left: 0;
}
.section-flag, nav#TOC .badge {
  font-size: 17px;
  font-weight: 300;
  font-family: Alegreya Sans SC;
}
.section-flag, nav#TOC > ul > li.top-li {
  margin-bottom: 0;
}
nav#TOC > ul > li.top-li {
  border-width: 2px 0 0 0;
  border-style: solid;
}
nav#TOC > ul > li.top-li:last-child {
  border-width: 2px 0 2px 0;
}
nav#TOC .badge {
  bottom: 0.13em;
  line-height: 1.2em;
  height: 1.2em;
  width: 2em;
  border-radius: 2px;
  background-color: #f0f0f0;
  box-shadow: 0px 1.0px 1.0px #aaa;
}
</style><script type="text/javascript" src="https://code.jquery.com/jquery-3.0.0.min.js"></script><link href="https://fonts.googleapis.com/css?family=Alegreya+Sans:400,100,100italic,300,300italic,400italic,500,500italic,700,700italic,800,800italic,900,900italic|Alegreya+Sans+SC:400,100,300,500,700,800,900,100italic,300italic,400italic,500italic,700italic,800italic,900italic|Alegreya+SC:400,400italic,700,700italic,900,900italic|Alegreya:400,700,900,400italic,700italic,900italic" rel="stylesheet" type="text/css"><link href="https://fonts.googleapis.com/css?family=Inconsolata:400,700" rel="stylesheet" type="text/css"><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_CHTML">MathJax.Hub.Config({ jax: ['output/CommonHTML'], CommonHTML: { scale: 100, linebreaks: {automatic: false}, mtextFontInherit: true} });</script><link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.5.0/css/font-awesome.min.css"><script type="text/javascript">// Generated by CoffeeScript 1.11.1
(function() {
  $(function() {
    return console.log("HELLO FROM DEMO!");
  });

}).call(this);
</script><script type="text/javascript">// Generated by CoffeeScript 1.11.1
(function() {
  var hide_proof, main, show_proof;

  hide_proof = function(section) {
    var clone, div, header, id, new_paragraph;
    clone = section.clone();
    id = section.attr("id");
    clone.attr({
      id: id + "---"
    });
    div = $("<div></div>");
    div.css({
      display: "none"
    });
    div.append(clone);
    header = section.find("h3, h4, h5, h6").first().clone();
    new_paragraph = $("<div class='p'></div>").append(header);
    new_paragraph.append("<i class='fa fa-caret-down expand' style='float:right;cursor:pointer;'></i>");
    section.empty();
    section.append(new_paragraph);
    section.append(div);
    return section.find("i.expand").on("click", (function(section) {
      return function() {
        return show_proof(section);
      };
    })(section));
  };

  show_proof = function(section) {
    var tombstone;
    section.children().first().remove();
    section.html(section.children().first().html());
    tombstone = section.find(".tombstone");
    tombstone.css({
      cursor: "pointer"
    });
    return tombstone.on("click", (function(section) {
      return function() {
        return hide_proof(section);
      };
    })(section));
  };

  main = function() {
    var header, i, j, len, len1, proof_sections, ref, results, section, sections, text;
    sections = $("section");
    proof_sections = [];
    for (i = 0, len = sections.length; i < len; i++) {
      section = sections[i];
      header = $(section).find("h1, h2, h3, h4, h5, h6").first();
      if (header.length && ((ref = header.prop("tagName")) === "H3" || ref === "H4" || ref === "H5" || ref === "H6")) {
        text = header.text();
        if (text.slice(0, 5) === "Proof") {
          proof_sections.push($(section));
        }
      }
    }
    results = [];
    for (j = 0, len1 = proof_sections.length; j < len1; j++) {
      section = proof_sections[j];
      results.push(hide_proof(section));
    }
    return results;
  };

  $(main);

}).call(this);
</script></head>
<body>
<main>
<header>
<h1 class="title"><a href="#">Binary Coding</a></h1>

<h2 class="author">
By <a href="Sebastien.Boisgerault@mines-paristech.fr">Sébastien Boisgérault</a>, <a href="http://www.mines-paristech.fr/">Mines ParisTech</a>
</h2> 

<h3 class="date">February 13, 2017</h3>
</header>
<section id="contents" class="level1"><h1><a href="#contents">Contents</a></h1><nav id="TOC">
<ul>
<li class="top-li"><p class="section-flag">section 1</p><a href="#binary-data">Binary Data</a><ul>
<li><a href="#bits">Bits</a><ul>
<li><a href="#binary-digits-and-numbers">Binary Digits and Numbers</a></li>
<li><a href="#bytes-and-words">Bytes and Words</a></li>
</ul></li>
<li><a href="#integers">Integers</a><ul>
<li><a href="#unsigned-integers">Unsigned Integers</a></li>
<li><a href="#signed-integers">Signed Integers</a></li>
<li><a href="#network-order">Network Order</a></li>
</ul></li>
</ul></li>
<li class="top-li"><p class="section-flag">section 2</p><a href="#information-theory-and-variable-length-codes">Information Theory and Variable-Length Codes</a><ul>
<li><a href="#entropy-from-first-principles">Entropy from first principles</a><ul>
<li></li>
<li><a href="#password-strength-measured-by-entropy">Password Strength measured by Entropy</a></li>
</ul></li>
<li><a href="#alphabets-symbol-codes-stream-codes">Alphabets, Symbol Codes, Stream Codes</a><ul>
<li></li>
<li><a href="#prefix-codes">Prefix Codes</a></li>
<li><a href="#example-of-variable-length-code-unicode-and-utf-8">Example of variable-length code: Unicode and utf-8</a></li>
<li><a href="#prefix-codes-representation">Prefix Codes Representation</a></li>
<li><span class="badge">pro<span></span></span><a href="#proposition--kraft-converse-theorem.">Kraft Converse Theorem</a></li>
<li></li>
</ul></li>
<li><a href="#optimal-length-coding">Optimal Length Coding</a><ul>
<li><a href="#average-bit-length-bounds">Average Bit Length – Bounds</a></li>
<li><span class="badge">pro<span></span></span><a href="#proposition.">Proposition</a></li>
<li></li>
<li><a href="#algorithm-and-implementation-of-huffman-coding">Algorithm and Implementation of Huffman Coding</a></li>
<li><a href="#optimality-of-the-huffman-algorithm">Optimality of the Huffman algorithm</a></li>
<li><a href="#switching-nodes.">Switching nodes.</a></li>
<li><a href="#length-optimality.">Length optimality.</a></li>
<li><a href="#example-brainfuck-spoon-and-fork">Example: Brainfuck, Spoon and Fork</a></li>
</ul></li>
<li><a href="#golomb-rice-coding">Golomb-Rice Coding</a><ul>
<li><a href="#optimality-of-unary-coding-reduced-sources">Optimality of Unary Coding – Reduced Sources</a></li>
<li><a href="#optimal-coding-of-geometric-distribution---rice-coding">Optimal Coding of Geometric Distribution - Rice Coding</a></li>
<li><a href="#rice-coding">Rice Coding</a></li>
<li><a href="#implementation">Implementation</a></li>
</ul></li>
</ul></li>
<li class="top-li"><p class="section-flag">section 3</p><a href="#notes">Notes</a></li></ul>
</nav></section>
<section id="binary-data" class="level1">
<h1><a href="#binary-data">Binary Data</a></h1>
<p>Digital audio data – stored in a file system, in a compact disc, transmitted over a network, etc. – always end up coded as binary data, that is a sequence of bits.</p>
<section id="bits" class="level2">
<h2><a href="#bits">Bits</a></h2>
<section id="binary-digits-and-numbers" class="level3">

<div class="p"><h3><a href="#binary-digits-and-numbers">Binary Digits and Numbers</a></h3>The term <strong>“bit”</strong> was coined in 1948 by the statistician John Tukey as a contraction of <strong>binary digit</strong>.</div>
<figure>
<img src="images/Tukey.png" alt="John Wilder Tukey (June 16, 1915 – July 26, 2000) was an American statistician best known for development of the FFT algorithm and box plot – source: http://en.wikipedia.org/wiki/John_Tukey" style="width: 100%;"><figcaption><strong>John Wilder Tukey</strong> (June 16, 1915 – July 26, 2000) was an American statistician best known for development of the FFT algorithm and box plot – source: <a href="http://en.wikipedia.org/wiki/John_Tukey" class="uri">http://en.wikipedia.org/wiki/John_Tukey</a></figcaption>
</figure>
<p>It therefore refers to a numeral symbol in base 2: either <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>. Several binary digits may be used to form <strong>binary numbers</strong> in order to represent arbitrary integers. In the decimal system (base <span class="math inline">\(10\)</span>), “<span class="math inline">\(42\)</span>” denotes <span class="math inline">\(4 \times 10^1 + 4 \times 10^0\)</span> ; similarly, in base 2, <span class="math display">\[b_0 b_1 \cdots b_{n-1} \; \mbox{ where } \; b_i \in \{0,1\}\]</span> represents the integer <span class="math display">\[
  b_0 \times 2^{n-1} + b_1 \times 2^{n-1} + \cdots + b_{n-1} \times 2^0.
  \]</span> As an example the number 42 (decimal representation), equal to <span class="math inline">\(1 \times 2^5 + 0 \times 2^4 + 1 \times 2^3 + 0 \times 2^2 + 1 \times 2^1 + 0 \times 2^0,\)</span> has the binary representation <span class="math inline">\(101010\)</span>.</p>
<figure>
<img src="images/1_to_10.png" alt="http://xkcd.com/953/" style="width: 53.14846513750999%;"><figcaption><a href="http://xkcd.com/953/" class="uri">http://xkcd.com/953/</a></figcaption>
</figure>
<p>The term “digit” used in the construction of “bit” is somehow self-contradictory as the etymology of “digit” refers to the base ten:</p>
<blockquote>
<p>The name “digit” comes from the fact that the 10 digits (ancient Latin digita meaning fingers) of the hands correspond to the 10 symbols of the common base 10 number system, i.e.&nbsp;the decimal (ancient Latin adjective dec. meaning ten) digits.</p>
<p><a href="http://en.wikipedia.org/wiki/Numerical_digit" class="uri">http://en.wikipedia.org/wiki/Numerical_digit</a></p>
</blockquote>
<p>In Python, the decimal notation is obviously available to define number litterals, but we also may use the prefix <code>0b</code> to define a number with its binary representation. The built-in function <code>bin</code> returns the binary representation of a number as a string:</p>
<pre><code>&gt;&gt;&gt; 42
42
&gt;&gt;&gt; 0b101010
42
&gt;&gt;&gt; bin(42)
'0b101010'</code></pre>
<p>Note that there is not a unique binary representation of an integer: one can add as many leading zeros without changing the value of the integer. The function <code>bin</code> uses a <em>canonical</em> representation of the binary value that uses the minimum number of digits necessary to represent the binary value … except for <code>0</code> which is represented as <code>0b0</code> and not <code>0b</code> ! Except for this particular value, the first binary digit (after the prefix <code>0b</code>) will always be <code>1</code>. However, sequences of bits beginning with (possibly) multiple <span class="math inline">\(0\)</span>’s are valid: <code>0b00101010</code> is a valid definition of <span class="math inline">\(42\)</span>. Finally, note that negative integers may also be described within this system: <span class="math inline">\(-42\)</span> is for example denoted as <code>-0b101010</code>.</p>
<p>Several arithmetic operators in Python (operation on numbers), for example <code>&lt;&lt;</code>, <code>&gt;&gt;</code>, <code>|</code>, <code>^</code>, <code>&amp;</code>, are far simpler to understand when their integer arguments are in binary representation. Consider:</p>
<pre><code>&gt;&gt;&gt; print 42 &lt;&lt; 3
336
&gt;&gt;&gt; print 42 &gt;&gt; 2
10
&gt;&gt;&gt; print 42 | 7
47
&gt;&gt;&gt; print 42 ^ 7
45
&gt;&gt;&gt; print 42 &amp; 7
2</code></pre>
<p>versus</p>
<pre><code>&gt;&gt;&gt; print bin(0b101010 &lt;&lt; 3)
0b101010000
&gt;&gt;&gt; print bin(0b101010 &gt;&gt; 2)
0b1010
&gt;&gt;&gt; print bin(0b101010 | 0b111)
0b101111
&gt;&gt;&gt; print bin(0b101010 ^ 0b111)
0b101101
&gt;&gt;&gt; print bin(0b101010 &amp; 0b111)
0b10</code></pre>
<p>To summarize:</p>
<ul>
<li><p><code>&lt;&lt; n</code> is the <strong>left shift by <code>n</code></strong>: bits are shifted on the left by <code>n</code> places, the holes being filled with zeros,</p></li>
<li><p><code>&gt;&gt; n</code> is the <strong>right shift by <code>n</code></strong>: bits are shifted on the right by <code>n</code> places, bits in excess being dropped,</p></li>
<li><p><code>|</code> is the <strong>bitwise or</strong>: the bits of the two binary numbers are combined place-by-place, the resulting bit being 0 if both bits are 0, and 1 otherwise,</p></li>
<li><p><code>^</code> is the <strong>bitwise exclusive or (xor)</strong>:<br>
the bits of the two binary numbers are combined place-by-place, the resulting bit being 1 if exactly one of the bits is 1, and 0 otherwise,</p></li>
<li><p><code>&amp;</code> is the <strong>bitwise and</strong>: the bits of the two binary numbers are combined place-by-place, the resulting bit being 1 if both bits are 1, and 0 otherwise.</p></li>
</ul>
</section>
<section id="bytes-and-words" class="level3">

<div class="p"><h3><a href="#bytes-and-words">Bytes and Words</a></h3>In many application contexts, we consider binary data as sequences of <em>groups</em> of a fixed number of bits named <strong>bytes</strong>. Consider for example that every data in computer memory is accessed through pointers and that a pointer gives the location of a byte, not of an individual bit. All the same, filesystems store data into files that contain an entire number of bytes. Despite several historical choices of the size of the byte (due to different hardware architectures), nowadays, the <em>de facto</em> size for the byte is 8 bits, that is the byte is an <strong>octet</strong>. In the sequel, we will drop the octet term entirely and use byte instead.</div>
<p>Bytes are sometimes grouped into even larger structures, called <strong>words</strong>, that may group 2, 3, 4, etc. bytes. The choice of the size of a word often reflect the design of a specific hardware architecture<a href="#fn1" class="footnoteRef" id="fnref1"><sup>1</sup></a>.</p>
<section id="alternate-representations---hexadecimal-and-ascii" class="level4">

<div class="p"><h4><a href="#alternate-representations---hexadecimal-and-ascii">Alternate Representations - Hexadecimal and ASCII</a></h4>The hexadecimal representation of an integer uses the base 16. As a consequence, any byte may be represented by an hexadecimal number with two digits. This representation, being more compact than the binary, often makes binary data easier to interpret by human beings. The 10 first hexadecimal digits are represented by the symbols <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(3\)</span>, <span class="math inline">\(4\)</span>, <span class="math inline">\(5\)</span>, <span class="math inline">\(6\)</span>, <span class="math inline">\(7\)</span>, <span class="math inline">\(8\)</span>, <span class="math inline">\(9\)</span> and the <span class="math inline">\(5\)</span> following (that correspond to the decimal value <span class="math inline">\(10\)</span>, <span class="math inline">\(11\)</span>, <span class="math inline">\(12\)</span>, <span class="math inline">\(13\)</span>, <span class="math inline">\(14\)</span>, <span class="math inline">\(15\)</span>) are a, b, c, d, e, f (sometimes capitalized).</div>
<p>As an example, here is the representation of a few numbers in decimal, binary and hexadecimal form:</p>
<div class="table"><table>
<caption>decimal, binary and hexadecimal representation of small integers.</caption>
<thead>
<tr class="header">
<th style="text-align: right;">decimal</th>
<th style="text-align: right;">binary</th>
<th style="text-align: right;">hexadecimal</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
<td style="text-align: right;">0</td>
</tr>
<tr class="even">
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
<td style="text-align: right;">1</td>
</tr>
<tr class="odd">
<td style="text-align: right;">2</td>
<td style="text-align: right;">10</td>
<td style="text-align: right;">2</td>
</tr>
<tr class="even">
<td style="text-align: right;">3</td>
<td style="text-align: right;">11</td>
<td style="text-align: right;">3</td>
</tr>
<tr class="odd">
<td style="text-align: right;">4</td>
<td style="text-align: right;">100</td>
<td style="text-align: right;">4</td>
</tr>
<tr class="even">
<td style="text-align: right;">5</td>
<td style="text-align: right;">101</td>
<td style="text-align: right;">5</td>
</tr>
<tr class="odd">
<td style="text-align: right;">6</td>
<td style="text-align: right;">110</td>
<td style="text-align: right;">6</td>
</tr>
<tr class="even">
<td style="text-align: right;">7</td>
<td style="text-align: right;">111</td>
<td style="text-align: right;">7</td>
</tr>
<tr class="odd">
<td style="text-align: right;">8</td>
<td style="text-align: right;">1000</td>
<td style="text-align: right;">8</td>
</tr>
<tr class="even">
<td style="text-align: right;">9</td>
<td style="text-align: right;">1001</td>
<td style="text-align: right;">9</td>
</tr>
<tr class="odd">
<td style="text-align: right;">10</td>
<td style="text-align: right;">1010</td>
<td style="text-align: right;">A</td>
</tr>
<tr class="even">
<td style="text-align: right;">11</td>
<td style="text-align: right;">1100</td>
<td style="text-align: right;">B</td>
</tr>
<tr class="odd">
<td style="text-align: right;">15</td>
<td style="text-align: right;">11111</td>
<td style="text-align: right;">F</td>
</tr>
<tr class="even">
<td style="text-align: right;">16</td>
<td style="text-align: right;">100000</td>
<td style="text-align: right;">10</td>
</tr>
<tr class="odd">
<td style="text-align: right;">42</td>
<td style="text-align: right;">101010</td>
<td style="text-align: right;">2A</td>
</tr>
</tbody>
</table></div>
<p>Integer literals in hexadecimal notation are supported in Python with the <code>0x</code> prefix. For example</p>
<pre><code>&gt;&gt;&gt; 0x10
16
&gt;&gt;&gt; 0xff
255
&gt;&gt;&gt; 0xcaffe
831486</code></pre>
<p>In Python 2.x, binary data is also naturally represented as <strong>strings</strong>, instances of the type <code>str</code> ; reading data from file objects or writing into them is made via a string representation. String are therefore used as the same time to describe text – even if in this case the <strong>unicode strings</strong> of type <code>unicode</code> are more appropriate – and binary data.</p>
<p>Strings are delimited with quotes. Within the quotes, a byte may be denoted a symbol (letter, digits, punctuation marks, etc.) – in which case the byte value is the ASCII code of the symbol – or by an escape sequence <code>\x??</code> where <code>??</code> is the hexadecimal representation of the byte. The latter case is handy when the symbol that would represent the byte is not printable. For example, <code>"S\xc3\xa9bastien"</code> is the string that represent the text “Sébastien” in the utf-8 text encoding: the e acute does not belong to the set of ASCII printable characters and is represented by the combination of the bytes <code>0xc3</code> and <code>0xa9</code>.</p>
<p>Consider the string made of all characters whose code increases from <span class="math inline">\(0\)</span> to <span class="math inline">\(255\)</span>. We write it to a file and use the canonical output of the <code>hexdump</code> command to display to file content. The middle columns contain the hexadecimal representation of the data and the right column the character representation. The symbols outside of the printable characters range (hexadecimal 20 to 7F) are replaced with dots.</p>
<pre><code>&gt;&gt;&gt; string = "".join([chr(i) for i in range(256)])
&gt;&gt;&gt; file = open("file.txt", "w")
&gt;&gt;&gt; file.write(string)
&gt;&gt;&gt; file.close()
&gt;&gt;&gt; import os
&gt;&gt;&gt; e = os.system("hexdump -C file.txt")
00000000  00 01 02 03 04 05 06 07  08 09 0a 0b 0c 0d 0e 0f  |................|
00000010  10 11 12 13 14 15 16 17  18 19 1a 1b 1c 1d 1e 1f  |................|
00000020  20 21 22 23 24 25 26 27  28 29 2a 2b 2c 2d 2e 2f  | !"#$%&amp;'()*+,-./|
00000030  30 31 32 33 34 35 36 37  38 39 3a 3b 3c 3d 3e 3f  |0123456789:;&lt;=&gt;?|
00000040  40 41 42 43 44 45 46 47  48 49 4a 4b 4c 4d 4e 4f  |@ABCDEFGHIJKLMNO|
00000050  50 51 52 53 54 55 56 57  58 59 5a 5b 5c 5d 5e 5f  |PQRSTUVWXYZ[\]^_|
00000060  60 61 62 63 64 65 66 67  68 69 6a 6b 6c 6d 6e 6f  |`abcdefghijklmno|
00000070  70 71 72 73 74 75 76 77  78 79 7a 7b 7c 7d 7e 7f  |pqrstuvwxyz{|}~.|
00000080  80 81 82 83 84 85 86 87  88 89 8a 8b 8c 8d 8e 8f  |................|
00000090  90 91 92 93 94 95 96 97  98 99 9a 9b 9c 9d 9e 9f  |................|
000000a0  a0 a1 a2 a3 a4 a5 a6 a7  a8 a9 aa ab ac ad ae af  |................|
000000b0  b0 b1 b2 b3 b4 b5 b6 b7  b8 b9 ba bb bc bd be bf  |................|
000000c0  c0 c1 c2 c3 c4 c5 c6 c7  c8 c9 ca cb cc cd ce cf  |................|
000000d0  d0 d1 d2 d3 d4 d5 d6 d7  d8 d9 da db dc dd de df  |................|
000000e0  e0 e1 e2 e3 e4 e5 e6 e7  e8 e9 ea eb ec ed ee ef  |................|
000000f0  f0 f1 f2 f3 f4 f5 f6 f7  f8 f9 fa fb fc fd fe ff  |................|</code></pre>
<p>The character representation of most binary data that is not text is going to be meaningless, apart from text tags used to identify the nature of the file, or specific chunks of text data within a heterogeneous data content. For example, the <code>hexdump</code> of the start of a WAVE audio file may look like:</p>
<pre><code>&gt;&gt;&gt; e = os.system(r"hexdump -C -n160 You\ Wanna\ Have\ Babies.wav")
00000000  52 49 46 46 d8 cf 06 00  57 41 56 45 66 6d 74 20  |RIFF....WAVEfmt |
00000010  10 00 00 00 01 00 02 00  44 ac 00 00 10 b1 02 00  |........D.......|
00000020  04 00 10 00 4c 49 53 54  18 00 00 00 49 4e 46 4f  |....LIST....INFO|
00000030  49 41 52 54 0c 00 00 00  4a 6f 68 6e 20 43 6c 65  |IART....John Cle|
00000040  65 73 65 00 64 61 74 61  94 cf 06 00 28 ff 2b ff  |ese.data....(.+.|
00000050  2b ff 2f ff 34 ff 37 ff  3f ff 3e ff 46 ff 41 ff  |+./.4.7.?.&gt;.F.A.|
00000060  48 ff 43 ff 41 ff 3f ff  32 ff 30 ff 15 ff 13 ff  |H.C.A.?.2.0.....|
00000070  f1 fe f0 fe cf fe d1 fe  bf fe c1 fe c1 fe c1 fe  |................|
00000080  cd fe d0 fe de fe e4 fe  f5 fe f9 fe 0c ff 0f ff  |................|
00000090  24 ff 26 ff 35 ff 35 ff  38 ff 3b ff 2c ff 32 ff  |$.&amp;.5.5.8.;.,.2.|
000000a0  1c ff 22 ff 13 ff 16 ff  1b ff 1a ff 36 ff 38 ff  |..".........6.8.|
000000b0  5f ff 65 ff 94 ff 96 ff  ce ff ca ff ff ff 00 00  |_.e.............|
000000c0  2b 00 2e 00 4b 00 4e 00  59 00 5e 00 57 00 5b 00  |+...K.N.Y.^.W.[.|
000000d0  4a 00 4c 00 43 00 41 00  43 00 42 00 4b 00 4f 00  |J.L.C.A.C.B.K.O.|
000000e0  59 00 5a 00 64 00 62 00  63 00 64 00 5b 00 5d 00  |Y.Z.d.b.c.d.[.].|
000000f0  4f 00 4d 00 3d 00 38 00  1c 00 1b 00 f5 ff fa ff  |O.M.=.8.........|
00000100  ce ff d4 ff a8 ff ae ff  7e ff 84 ff 4c ff 52 ff  |........~...L.R.|
00000110  18 ff 1b ff ee fe f0 fe  d6 fe d8 fe d0 fe d0 fe  |................|
00000120  cd fe cf fe c8 fe ca fe  c4 fe c0 fe c8 fe c3 fe  |................|
00000130  d4 fe d3 fe e8 fe ea fe  fd fe ff fe 08 ff 08 ff  |................|
00000140</code></pre>
</section>
</section>
</section>
<section id="integers" class="level2">
<h2><a href="#integers">Integers</a></h2>
<p>NumPy provide 8 different types to describe integers: <code>uint8</code>, <code>uint16</code>, <code>uint32</code>, <code>uint64</code>, <code>int8</code>, <code>int16</code>, <code>int32</code> and <code>int64</code>. The digits at the end of the type name denote the size in bits of the representation: all these representations are <strong>fixed-size</strong> and use an entire number of bytes so that they may be handled efficiently by the hardware. The type whose name start with a “u” are unsigned, they represent only non-negative integers ; the others are signed, they may represent negative integers as well.</p>
<p>We will explain how these types represent integers as binary data. We will use the <code>bitstream</code> library to illustrate this ; more specifically, we will only use its ability to handles bits as sequence of booleans and build on top of that the functions needed to manage the integer types.</p>
<section id="unsigned-integers" class="level3">

<div class="p"><h3><a href="#unsigned-integers">Unsigned Integers</a></h3>The non-negative integers that may be described in one byte are in the range <span class="math inline">\(0, 1, ..., 255\)</span>. Writing them in a binary stream one bit at a time may be done like that:</div>
<pre><code>def write_uint8(stream, integers):
    integers = array(integers)
    for integer in integers:
        mask = 0b10000000
        while mask != 0:
            stream.write((integer &amp; mask) != 0)
            mask = mask &gt;&gt; 1</code></pre>
<p>The use of the statement <code>integers = array(integers)</code> ensures that single integers, lists of integers and array of integers will all be handled as arrays.</p>
<p>Reading unsigned 8-bit integers from a stream is even simpler:</p>
<pre><code>def read_uint8(stream, n=None):
    if n is None:
        integer = 0
        for _ in range(8):
            integer = (integer &lt;&lt; 1) + int(stream.read(bool))
        return integer
    else:
        return array([read_uint8(stream) for _ in range(n)], uint8)</code></pre>
<p>This code can easily be generalized to handle 16-bits or 32-bits integers.</p>
<p>The <code>audio.bitstream</code> library actually supports these types already, with quite a few extra features – error handling, efficient vectorization, and static typing for extra performance via <a href="http://cython.org/">Cython</a>.</p>
<p>If that was not already done, we could register the above encoder and decoder via</p>
<pre><code>bitstream.register(uint8, reader=read_uint8, writer=write_uint8)</code></pre>
<p>The following Python session demonstrates a few ways to write 8-bits unsigned to streams and read them from streams.</p>
<pre><code>&gt;&gt;&gt; stream = BitStream()
&gt;&gt;&gt; stream.write(uint8(0))
&gt;&gt;&gt; stream.write(15, uint8)
&gt;&gt;&gt; write_uint8(stream, 255)
&gt;&gt;&gt; print stream
000000000000111111111111
&gt;&gt;&gt; stream.write([0, 15], uint8)
&gt;&gt;&gt; stream.write([uint8(255), uint8(0)])
&gt;&gt;&gt; stream.write(array([15, 255], uint8))
&gt;&gt;&gt; print stream
000000000000111111111111000000000000111111111111000000000000111111111111
&gt;&gt;&gt; stream.read(uint8)
0
&gt;&gt;&gt; stream.read(uint8, 2)
array([ 15, 255], dtype=uint8)
&gt;&gt;&gt; stream.read(uint8, inf)
array([  0,  15, 255,   0,  15, 255], dtype=uint8)</code></pre>
<p>The first form to write integers – <code>stream.write(uint8(0))</code> – automatically detects the type of the argument, the second – <code>stream.write(15, uint8)</code> – uses an explicit type argument and is slightly faster. The third one that calls the writer directly and bypasses the call to <code>stream.write</code> – <code>write_uint8(stream, 127)</code> – is again slightly faster. But the real way to make a difference performance-wise is to vectorize these calls and write several integers at once, either lists of integers – as in the calls to <code>stream.write([0, 15], uint8)</code> or <code>stream.write([uint8(255), uint8(0)])</code> – or better yet write NumPy arrays with data type <code>uint8</code> as in the call <code>stream.write([15, 255], uint8)</code>.</p>
<p>Reading such integers from a stream may be done one integer at at time – as in the call <code>stream.read(uint8)</code> – or many at the same time, the data being packed into NumPy arrays – as in <code>stream.read(uint8, 2)</code>, the second argument being the number of integers.</p>
</section>
<section id="signed-integers" class="level3">

<div class="p"><h3><a href="#signed-integers">Signed Integers</a></h3>A first approach to the coding of 8-bit signed integers would be to use the first bit to code the sign and the remaining 7 bits to code the absolute value of the integer.</div>
<pre><code>def write_int8(stream, integers):
    integers = array(integers)
    for integer in integers:
        stream.write(integer &lt; 0)
        integer = abs(integer)
        mask = 0b1000000
        while mask != 0:
            stream.write((integer &amp; mask) != 0)
            mask = mask &gt;&gt; 1
    stream.write</code></pre>
<p>In this scheme, the code <code>1000000</code> is never used (it would correspond to <span class="math inline">\(-0\)</span>), and therefore we could code <span class="math inline">\(255\)</span> different integers, from <span class="math inline">\(-127\)</span> to <span class="math inline">\(127\)</span>. By shifting the negative values by <span class="math inline">\(1\)</span> we could use <em>all</em> 8-bit codes and therefore encode the <span class="math inline">\(-128\)</span> to <span class="math inline">\(127\)</span> range. The modification would be:</p>
<pre><code>def write_int8(stream, integers):
    integers = array(integers)
    for integer in integers:
        if integer &lt; 0:
            negative = True
            integer = - integer - 1
        else:
            negative = False
        stream.write(negative)
        mask = 0b1000000
        while mask != 0:
            stream.write((integer &amp; mask) != 0)
            mask = mask &gt;&gt; 1</code></pre>
<p>The <em>actual</em> standard coding of 8-bit signed integers uses a slightly different scheme called <strong>two’s complement</strong>: conceptually, we encode the integer as in the previous scheme but as a last step, the bits other than the sign bits are inverted – 0 for 1, 1, for 0 – for the negative numbers. A careful examination of the code below shows that it implements effectively the desired scheme ; the line <code>2**8 - integer - 1</code> actually explains the name of this scheme:</p>
<pre><code>def write_int8(stream, integers):
    integers = array(integers)
    for integer in integers:
        if integer &lt; 0:
            integer = 2**8 - integer - 1
        mask = 0b10000000
        while mask != 0:
            stream.write((integer &amp; mask) != 0)
            mask = mask &gt;&gt; 1</code></pre>
<p>The motivation behind two’s complement schemes is that addition correctly works between signed integers, without any special handling of the bit sign<a href="#fn2" class="footnoteRef" id="fnref2"><sup>2</sup></a>.</p>
<p>Let’s see the result of this encoding on a few examples:</p>
<pre><code>&gt;&gt;&gt; BitStream(0, int8)
00000000
&gt;&gt;&gt; BitStream(127, int8)
01111111
&gt;&gt;&gt; BitStream(-128, int8)
10000000
&gt;&gt;&gt; BitStream(-127, int8)
10000001
&gt;&gt;&gt; BitStream(-3, int8)
11111101
&gt;&gt;&gt; BitStream(-2, int8)
11111110
&gt;&gt;&gt; BitStream(-1, int8)
11111111</code></pre>
</section>
<section id="network-order" class="level3">

<div class="p"><h3><a href="#network-order">Network Order</a></h3>Consider the integer 3882 ; we need at least two bytes to encode this number in an unsigned fixed multi-byte scheme. The <code>audio.bitstream</code> modulde represents this integer as <code>0000111100101010</code>: the integer 3882 is equal to <span class="math inline">\(15 \times 2^8 + 42\)</span>. We have encoded the 8-bit unsigned integer <span class="math inline">\(15\)</span> – the <strong>most significant bits</strong> <code>00001111</code> – <em>first</em> and then the <strong>least significant bits</strong> <code>00101010</code>. This scheme is called the <strong>big endian</strong> ordering and this is what the <code>audio.bitstream</code> module support by default. The opposite scheme – the <strong>little endian</strong> ordering – would encode <span class="math inline">\(42\)</span> first and then <span class="math inline">\(15\)</span>, and then the bit content would be <code>0010101000001111</code>.</div>
<p>NumPy provides a handy method on its multi-byte integer types: <code>newbyteorder</code>. It allows to switch from a little endian representation to a big endian and reciprocally. See for example:</p>
<pre><code>&gt;&gt;&gt; BitStream(uint16(42))
0000000000101010
&gt;&gt;&gt; BitStream(uint16(42).newbyteorder())
0010101000000000
&gt;&gt;&gt; BitStream(uint32(42))
00000000000000000000000000101010
&gt;&gt;&gt; BitStream(uint32(42).newbyteorder())
00101010000000000000000000000000</code></pre>
<p>So reading an integer encoded in little endian representation is done in two steps: read it (as if it was encoded with the big endian representation) and then use the method <code>newbyteorder</code> on the result.</p>
</section>
</section>
</section>
<section id="information-theory-and-variable-length-codes" class="level1">
<h1><a href="#information-theory-and-variable-length-codes">Information Theory and Variable-Length Codes</a></h1>
<p>In this section, we introduce the modelling of <strong>sources</strong> or <strong>channels</strong> as random generators of symbols and measure the quantity of information they have. We’ll see later – as anyone can guess – that such a measure directly influences the size of the binary data necessary to encode such sources.</p>
<section id="entropy-from-first-principles" class="level2">
<h2><a href="#entropy-from-first-principles">Entropy from first principles</a></h2>
<p>The <strong>(Shannon) information content</strong> of an event <span class="math inline">\(E\)</span> is: <span class="math display">\[
  I(E) = - \log_2 P(E)
  \]</span></p>
<figure>
<img src="images/shannon-board.png" alt="Claude E. Shannon (April 30, 1916 – February 24, 2001) was an American mathematician, electronic engineer, and cryptographer known as the father of information theory." style="width: 71.85369944698941%;"><figcaption><strong>Claude E. Shannon</strong> (April 30, 1916 – February 24, 2001) was an American mathematician, electronic engineer, and cryptographer known as “the father of information theory”.</figcaption>
</figure>
<figure>
<img src="images/information_content.svg" alt="Information Content"><figcaption>Information Content</figcaption>
</figure>
<p>This expression may actually be derived from a simple set of axioms:</p>
<ul>
<li><p><strong>Positivity:</strong> the information content has non-negative – finite or infinite – values,</p></li>
<li><p><strong>Neutrality:</strong> the information content of an event only depends on the probability of the event,</p></li>
<li><p><strong>Normalization:</strong> the information content of an event with probability <span class="math inline">\(1/2\)</span> is <span class="math inline">\(1\)</span>,</p></li>
<li><p><strong>Additivity:</strong> the information content of a pair of independent events is the sum of the information content of the events.</p></li>
</ul>
<p>The positivity of the information content is a natural assumption. The value <span class="math inline">\(+\infty\)</span> has to be allowed for a solution to the set of axioms to exist: it does correspond to events whose probability is <span class="math inline">\(0\)</span> ; they are so unlikely that their occurence brings an infinite amount of information. More generally, it’s easy to see, given the expression of the informatio content, that the less likely an event is, the bigger its information content is. The additivity axiom is also quite natural but one shall emphasize the necessity of the independence assumption. On the contrary, consider two events <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span> that are totally dependent: if the first occurs, we know for sure that the second also will. Then <span class="math inline">\(P(E_1\wedge E_2) = P(E_1) \times P(E_2 | E_1) = P(E_1)\)</span> and therefore, the information content <span class="math inline">\(I(E_1)\)</span> is equal to <span class="math inline">\(I(E_1\wedge E_2)\)</span>: the occurence of the second event brings no further information.</p>
<p>The normalization axiom is somehow arbitrary: we could have selected any finite positive value instead of <span class="math inline">\(1\)</span>, but this convention is the most convenient in the context of binary data. This axiom basically says that the probability that a random bit – whose values <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span> are equally likely – is <span class="math inline">\(1\)</span> (or <span class="math inline">\(0\)</span> for that matter) is <span class="math inline">\(1\)</span>. So if we imagine a memory block of <span class="math inline">\(n\)</span> bits whose values is random and independent, for any sequence of <span class="math inline">\(n\)</span> values in <span class="math inline">\(\{0,1\}\)</span>, the probability that the memory block has that precise state is <span class="math inline">\(n\times 1 = n\)</span>. With this convention, the information content unit corresponds to the number of bits ! Therefore, it makes sense to use the word “bit” as a basic measure of information content ; Shannon use the word with this meaning as soon as 1948, the year Tukey invented it.</p>
<p>Let’s now prove that the first four fundamental axioms imply that the information content of an event is given by the formula <span class="math inline">\(I(E) = - \log_2 P(E)\)</span>.</p>
<section id="proof." class="level3">

<div class="p"><h3><a href="#proof.">Proof.</a></h3>‌Let <span class="math inline">\(f:[0,1] \to [0, +\infty]\)</span> be such that for any event <span class="math inline">\(E\)</span>, we have <span class="math inline">\(I(E) = f(P(E))\)</span>. Let <span class="math inline">\(E_1\)</span> and <span class="math inline">\(E_2\)</span> be independent events with <span class="math inline">\(p_1 = P(E_1)\)</span> and <span class="math inline">\(p_2 = P(E_2)\)</span>. As <span class="math inline">\(P(E_1 \wedge E_2) = p_1 \times p_2\)</span>, we have <span class="math display">\[f(p_1 \times p_2) = I(E_1 \wedge E_2) = I(E_1) + I(E_2) = f(p_1) + f(p_2).\]</span> Hence the function <span class="math inline">\(g = f \circ \exp\)</span> is additive: for any <span class="math inline">\(x, y \in [-\infty, 0]\)</span>, <span class="math inline">\(g(x + y) = g(x) + g(y)\)</span>. As <span class="math inline">\(g\)</span> takes non-negative values, <span class="math inline">\(g\)</span> is non-increasing. Moreover, as <span class="math inline">\(g(- \log 2) = f(1/2) = 1\)</span>, it is finite on <span class="math inline">\((-\infty, 0]\)</span> and satisfies <span class="math inline">\(g(-\infty) = +\infty\)</span> and <span class="math inline">\(g(0) = 0\)</span>. It is also continous on <span class="math inline">\((-\infty, 0]\)</span>. As <span class="math inline">\(g(-p/q) = p/q \times g(-1)\)</span> for any <span class="math inline">\((p,q) \in \mathbb{N} \times \mathbb{N}^{*}\)</span>, by continuity, <span class="math inline">\(g(x) = -x g(-1)\)</span>: <span class="math inline">\(g\)</span> is homogeneous. Precisely, <span class="math inline">\(g(x) = -({x}/{\log 2}) g(- \log 2) = -{x}/{\log 2}\)</span> and therefore <span class="math inline">\(f(p) = g \circ \log (p) = - {\log p} / {\log 2} = - \log_2 p\)</span>. <span class="tombstone" style="float:right;">‌<span class="math inline">\(\blacksquare\)</span></span></div>

<!--
%**\footnotesize What about entropy for continuous variables ? Have a look at differential
%entropy ? (Wikipedia). Heuristic to get it as the "limit" of the continous
%expression ? Or transform the discrete definition into an integral and see
%the (obvious) generalisation ? See however \url{http://en.wikipedia.org/wiki/Limiting_density_of_discrete_points**:
%that may not work /  be the right concept. Had seen something about differential
%entropy being defined w.r.t. a reference measure where was that ?
%MacKay \cite{Mac03} has a short reference to this in the section
%*Infinite Precision*, p. 180 -- basically he says that entropy does not
%make sense (is infinite) for continous random variables, but that the {\em
%mutual information} does. Investigate that. Have a look at \cite{Gra90},
%there are elements on it but it was not what I was thinking about ... OK, 
%that was probably \url{http://octavia.zoology.washington.edu/teaching/429/lecturenotes/Lecture11.pdf} !
%See also \url{http://mathoverflow.net/questions/33088/entropy-of-a-general-prob-measure-closed}, 
%basically the *relative* entropy of $\nu$ wrt $\mu$ is -- provided $\nu << \mu$:
%  $$
%  H(\nu||\mu) = \int \frac{d\nu}{d\mu} \log \frac{\frac{d\nu}{d\mu}}{\int \frac{d\nu}{d\mu} \, d\mu} \, d\mu
%  $$
%Q: forgot a minus somewhere ? Nope. Note that if we handle probability measures
%only, we can simplify to:
%  $$
%  H(\nu||\mu) = - \int \log \frac{d\mu}{d\nu} \, d\nu
%  $$
%and then the proper litterature is found under the term of 
%"Kullback–Leibler divergence" (or information divergence/information gain) --
%see Wikipedia to begin with: KL measures the expected number of extra bits 
%required to code samples from $\nu$ when using a code based on $\mu$, rather 
%than using a code based on $\nu$ (mmm maybe the sign is an issue after all,
%I'd rather have $\nu$ being the code and $\mu$ the data.}

-->
</section><section><p>Let <span class="math inline">\(X\)</span> be a discrete random variable. The <strong>entropy of <span class="math inline">\(X\)</span></strong> is the mean information content among all possible outcomes <span class="math display">\[
  H(X) = \mathbb{E} \left[ x \mapsto I(X=x)\right],
  \]</span> or explicitly <span class="math display">\[
  H(X) = - \sum_{x} P (X=x) \log_2 P(X=x)
  \]</span> If <span class="math inline">\(X\)</span> takes <span class="math inline">\(N\)</span> possible different values <span class="math inline">\(x_1\)</span> to <span class="math inline">\(x_N\)</span>, we can show that the entropy is maximal if <span class="math display">\[
  P(X=x_1) = \cdots = P(X=x_N) = 1/N
  \]</span> that is, if the variable <span class="math inline">\(X\)</span> is “totally random”, all of its values being equally likely. In this case, we have <span class="math display">\[
  H(X) = \log_2 N
  \]</span> In particular, if the random variable <span class="math inline">\(X\)</span> with values in <span class="math inline">\(\{0, 1, \cdots, 2^n-1\}\)</span> is a model for the state of a memory block of <span class="math inline">\(n\)</span> bits where all states are equally likely, then <span class="math display">\[
  H(X) = n
  \]</span> so once again, it makes sense to attach to the entropy a unit named “bits”.</p></section>

</section><section><section id="password-strength-measured-by-entropy" class="level3">

<div class="p"><h3><a href="#password-strength-measured-by-entropy">Password Strength measured by Entropy</a></h3>Given that entropy measures the quantity of information of a random symbol source, it is an interesting tool to measure the strength of a password. Let’s measure the entropy attached to the first password generation method described in the xkcd strip below; the algorithm generates passwords such as</div>
<pre><code>$ ./password.py 5
Ablatival8'
Horseplay&gt;4
greff0tome1*
beelzebub)4
Conta¡nments5.</code></pre>
<figure>
<img src="images/password_strength.png" alt="&quot;Tr0ub4dor\&amp;3&quot; or &quot;correct horse battery staple&quot; ? http://xkcd.com/936/" style="width: 100%;"><figcaption><code>"Tr0ub4dor\&amp;3"</code> or <code>"correct horse battery staple"</code> ? <a href="http://xkcd.com/936/" class="uri">http://xkcd.com/936/</a></figcaption>
</figure>
<p>Here are the steps that are reproduced:</p>
<ul>
<li><p><strong>long and uncommon base word.</strong> Pick at random an english word of at least 9 letters. The <code>words.py</code> utility gives us the number of such words:</p>
<pre><code>$ ./words.py "len(word) &gt;= 9"
67502</code></pre>
<p>At this stage, your pick may be:</p>
<pre><code>"troubador"</code></pre>
<p>If every word in the dictionary has the same chance to be picked, the entropy of your word generator so far is:</p>
<pre><code>&gt;&gt;&gt; log2(67502)
16.042642627599378</code></pre>
<p>So far our estimate closely matches the one from the xkcd comic strip. (16 grey “bit boxes” attached to the base random word).</p></li>
<li><p><strong>caps ?</strong> Capitalize the word – or don’t – and that randomly (each strategy is equally likely and independent of the previous pick).</p>
<p>You password at this stage may be:</p>
<pre><code>"Troubador"</code></pre>
<p>and the corresponding entropy:</p>
<pre><code>&gt;&gt;&gt; log2(67502) + 1
17.042642627599378</code></pre></li>
<li><p><strong>common substitutions.</strong> Let’s pretend that there is a flavor of the “leetspeak” language (see <a href="http://en.wikipedia.org/wiki/Leet" class="uri">http://en.wikipedia.org/wiki/Leet</a>) where every letter is uniquely represented by an alternate symbol that is unique and <em>not</em> a letter. For example <code>a</code> <span class="math inline">\(\rightarrow\)</span> <code>4</code>, <code>b</code> <span class="math inline">\(\rightarrow\)</span> <code>8</code>, <code>c</code> <span class="math inline">\(\rightarrow\)</span> <code>(</code>, <code>e</code> <span class="math inline">\(\rightarrow\)</span> <code>3</code>, <code>l</code> <span class="math inline">\(\rightarrow\)</span> <code>1</code>, <code>o</code> <span class="math inline">\(\rightarrow\)</span> <code>0</code>, <code>i</code> <span class="math inline">\(\rightarrow\)</span> <code>!</code>, <code>t</code> <span class="math inline">\(\rightarrow\)</span> <code>7</code>, <code>x</code> <span class="math inline">\(\rightarrow\)</span> <code>\%</code>, etc. so that <code>leet</code> for example is represented as <code>1337</code>. Replace letters in position 3, 6 and 8 by their corresponding leetspeak number – or don’t do it – randomly, the two options being equally likely and independent of the previous steps.</p>
<p>You password at this stage may be:</p>
<pre><code>"Tr0ub4dor"</code></pre>
<p>and the corresponding entropy:</p>
<pre><code>&gt;&gt;&gt; log2(67502) + 1 + 3
20.042642627599378</code></pre></li>
<li><p><strong>punctuation and numeral.</strong> Pick at random a non-letter, non-digit symbol among printable characters in the US-ASCII set. There are <span class="math inline">\(127\)</span> US-ASCII characters, <span class="math inline">\(95\)</span> of them are printable (codes <code>0x20</code> to <code>0x7E</code> (see <a href="http://en.wikipedia.org/wiki/ASCII#ASCII_printable_characters" class="uri">http://en.wikipedia.org/wiki/ASCII#ASCII_printable_characters</a>), that makes <span class="math inline">\(95\)</span> minus <span class="math inline">\(10\)</span> digits minus <span class="math inline">\(2 \times 26\)</span> letters – lower and upper case – that is <span class="math inline">\(33\)</span> possible symbols. Add a random digit symbol and optionally swap the punctuation and digit symbols at random.</p>
<p>You password at this stage may be:</p>
<pre><code> "Tr0ub4dor&amp;3"</code></pre>
<p>and the corresponding entropy:</p>
<pre><code>&gt;&gt;&gt; log2(67502) + 1 + 3 + log2(33) + log2(10) + 1
29.408964841845194</code></pre></li>
</ul>
</section></section>



</section><section><section id="alphabets-symbol-codes-stream-codes" class="level2">
<h2><a href="#alphabets-symbol-codes-stream-codes">Alphabets, Symbol Codes, Stream Codes</a></h2>
<p>An <strong>alphabet</strong> is a countable set of distinct symbols, meant to represent information. For example, the letters <code>"a"</code> to <code>"z"</code>, all unicode glyphs, the words of the English language, the non-negative integers, the binary values <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, etc.</p>
<p>Let <span class="math inline">\(\mathcal{A}\)</span> be an alphabet and <span class="math inline">\(n \in \mathbb{N}\)</span>. We denote <span class="math inline">\(\mathcal{A}^n\)</span> the set of all <span class="math inline">\(n\)</span>-uples with elements in <span class="math inline">\(\mathcal{A}\)</span>, <span class="math inline">\(\mathcal{A}^+\)</span> the set of all such non-empty <span class="math inline">\(n\)</span>-uples when <span class="math inline">\(n\)</span> varies and <span class="math inline">\(\mathcal{A}^{\ast}\)</span> the set of all such <span class="math inline">\(n\)</span>-uples, including the (empty) <span class="math inline">\(0\)</span>-uple denoted <span class="math inline">\(\epsilon\)</span>. <span class="math display">\[
  \mathcal{A}^n = \; \stackrel{\mbox{$n$ terms}}{\overbrace{\mathcal{A} \times  \cdots \times {\mathcal{A}}}},
  \; \; \mathcal{A}^+ = \bigcup_{n=1}^{+\infty} \mathcal{A}^n,
  \; \; \mathcal{A}^{\ast} = \{\epsilon\} \cup \mathcal{A}^+
  \]</span> Elements of <span class="math inline">\(\mathcal{A}^n\)</span> are most of the time denoted without the commas and parentheses, by simple juxtaposition of the symbols of the tuple as in <span class="math display">\[
  a_0 a_1 \cdots a_{n-1} \in \mathcal{A}^n
  \]</span> We also talk about <strong>sequences</strong> and <strong>(finite) streams</strong> when we refer to such <span class="math inline">\(n\)</span>-uples. The <strong>length</strong> of a stream <span class="math inline">\(a_0 \cdots a_n\)</span> is the number of symbols that it contains: <span class="math display">\[
  |a_0  \cdots a_{n-1}| = n
  \]</span> A <strong>(variable-length) (binary) symbol code</strong> <span class="math inline">\(c\)</span> is an mapping from an alphabet <span class="math inline">\(\mathcal{A}\)</span> to the set of non-empty finite binary streams <span class="math inline">\(\{0,1\}^+\)</span>. The code is of fixed length <span class="math inline">\(n\)</span> if <span class="math inline">\(c(a)\)</span> has length <span class="math inline">\(n\)</span> for any <span class="math inline">\(a \in \mathcal{A}\)</span>. We usually require the code to be injective, sometimes using the term <strong>non-ambigous code</strong> to emphasize this property. The elements of <span class="math inline">\(\{0,1\}^+\)</span> are named <strong>(binary) codes</strong>, and <strong>valid codes</strong> if they belong to the range of <span class="math inline">\(c\)</span>. Such mappings are characterized by: <span class="math display">\[
  c: \mathcal{A} \to \{0,1\}^+ \; \mbox{ such that } \;
  c(a) = c(b) \implies a = b
  \]</span> The non-ambiguity assumption makes the code uniquely decodable: given <span class="math inline">\(c(a)\)</span>, <span class="math inline">\(a\)</span> is identified uniquely and may be recovered. Symbol codes are usually defined to form <strong>stream codes</strong>: the mapping <span class="math inline">\(c\)</span> is extended from <span class="math inline">\(\mathcal{A}\)</span> to <span class="math inline">\(\mathcal{A}^+\)</span> by <span class="math display">\[
  c(a_0a_1 \cdots a_{n-1}) = c(a_0) c(a_1) \cdots c(a_{n-1})
  \]</span> The resulting mapping is still a code – with symbols in the alphabet <span class="math inline">\(\mathcal{A}^+\)</span> instead of <span class="math inline">\(\mathcal{A}\)</span>) – only if the extended mapping is still non-ambiguous or {bf self-delimiting}, that is, the unique decodability is preserved by the extension to streams. % In order for the stream code is non-ambiguous, the lengths of the valid codes have to satisfy the Kraft inequality: <span class="math display">\[
  K(c) = \sum_{a \in \mathcal{A}} 2^{-|c(a)|} \leq 1
  \]</span> A converse statement also holds, see later.</p>
<section id="proof.-1" class="level3">

<div class="p"><h3><a href="#proof.-1">Proof.</a></h3>‌Let <span class="math inline">\(c\)</span> be a symbol code for <span class="math inline">\(\mathcal{A}\)</span> whose stream code is non-ambiguous. For any integer <span class="math inline">\(n\)</span>, we have <span class="math display">\[
  K(c)^n = \sum_{a_0 \in \mathcal{A}} \cdots \sum_{a_{n-1} \in \mathcal{A}} 
        2^{-(|c(a_0)| + \cdots + |c(a_{n-1})|)} =
        \sum_{a \in \mathcal{A}^n} 
        2^{-|c(a)|}
  \]</span> Assume than <span class="math inline">\(\mathcal{A}\)</span> is finite and let <span class="math inline">\(L\)</span> be an upper bound for the code length of symbols in <span class="math inline">\(\mathcal{A}\)</span>. For any integer <span class="math inline">\(l\)</span>, there are at most <span class="math inline">\(2^l\)</span> distinct binary stream codes whose length is <span class="math inline">\(l\)</span> and therefore <span class="math display">\[
  K(c)^n = \sum_{l=n}^{nL} \sum_{{a \in \mathcal{A}^n}{, |c(a)|=l}} 2^{-l} 
        \leq \sum_{l=n}^{n L} 2^l \times 2^{-l} = n (L -1)+ 1
  \]</span> Passing to the limit on <span class="math inline">\(n\)</span> yields <span class="math inline">\(K(c) \leq 1\)</span>. If <span class="math inline">\(\mathcal{A}\)</span> is countable and <span class="math inline">\(K(c)&gt;1\)</span>, there is a finite subset <span class="math inline">\(\mathcal{A}'\)</span> of <span class="math inline">\(\mathcal{A}\)</span> such that <span class="math inline">\(c' = c|_{\mathcal{A}'}\)</span> satisfies <span class="math inline">\(K(c') &gt; 1\)</span>. Hence, <span class="math inline">\(c'\)</span> is ambiguous as a stream code and therefore <span class="math inline">\(c\)</span> also is. <span class="tombstone" style="float:right;">‌<span class="math inline">\(\blacksquare\)</span></span></div>
</section>





</section><section><section id="prefix-codes" class="level3">

<!--
%**[TODO: try to talk about delimiters strategies ?] \footnotesize

%{\bf [The comparison is lousy below because the string stuff is a fixed-length
%      scheme ... devising delimiter strategy for variable-length is HARDER.
%      Still we could think of something. Explain that delimiter strategy is
%      easy for fixedt-length code, not so for variable-length, but try
%      (diagonal argument ... wow dunno, that's a real mess with codes being concatenated !
%       No, u can't always "delimitify" a code, consider the code that uses every
%       8-bit-sequence: u can't have a delimiter for him. Or even the $0$ and $1$
%       code ! You'd have to "reserve" some space and then, how to exploit it ?), 
%      then realize it's a kind of prefix code, just not optimal at all !
%      Also consider the size + data scheme as a special case ?]**

%Note that there is an obvious strategy to deal with this the same issue:
%add to the alphabet a special symbol and a corresponding code that acts as a 
%delimiter to mark the end of a code and don't worry if the original code is
%or is not a prefix. That's for example how strings are dealt with in $C$
%(see `string.h` header): strings are NULL-terminated: 8 bits equal to 0
%act as a marker for the end of the string.

%$$
%SOH \rightarrow 00000001, \;
%STX \rightarrow 00000010, \;
%..., \;
%a \rightarrow 01100001, \;
%b \rightarrow 01100010, \cdots
%$$

%(source: <http://www.unicode.org/charts/PDF/U0000.pdf>)

%However, delimiter-based codes ARE a special case of prefix codes ... and
%they are generally less efficient in data representation/compactness.


%That is a nice property to have as in this case, a sequence of symbols may 
%be encoded directly by the concatenation of the symbols codes: no specific code 
%is required to act as a delimiter. }

-->
<div class="p"><h3><a href="#prefix-codes">Prefix Codes</a></h3>Unique decodability is necessary for a code to be useful. Still it does not make the code <em>easy</em> to use. Consider for example the alphabet <span class="math inline">\(\mathcal{A}= \{0,1,2,3\}\)</span> and the mapping <span class="math inline">\(\mathcal{A} \to \{0,1\}^+\)</span> <span class="math display">\[
  0 \rightarrow 0, \; 1 \rightarrow 01, \; 2 \rightarrow 011, \; 3 \rightarrow 0111 
  \]</span> It is a code and we can convince ourselves that it is uniquely decodable. However decoding a stream requires lookahead: consider the stream that start by <span class="math inline">\(010110 \cdots\)</span> and imagine a process that lets you discover the symbols one by one. At the first step, <span class="math inline">\(0\)</span>, you don’t know what the original first symbol is: it could be <span class="math inline">\(0\)</span>, or it could be the partial code for any of the original symbols. So, you have to pick the second symbol, and you can rule out <span class="math inline">\(0\)</span> but you still can’t decide, the first symbol could be <span class="math inline">\(1\)</span>, or <span class="math inline">\(2\)</span> or <span class="math inline">\(3\)</span>. Only the third symbol gives you the solution: the stream starts with <span class="math inline">\(010\)</span> and it can be the case only of the first symbol was <span class="math inline">\(1\)</span> (with code <span class="math inline">\(01\)</span>).</div>
<p>Given a possible symbol, here, with one step at most we can decide if a code is complete or partial but generally we can build uniquely decodable codes for which this limit is arbitrarily large.</p>
<p>Consider on the contrary the code <span class="math display">\[
  0 \rightarrow 0, \; 1 \rightarrow 10, \; 2 \rightarrow 110, \; 3 \rightarrow 1110 
  \]</span> As soon as you receive a <span class="math inline">\(0\)</span>, the code is complete and you know that you can decode a symbol. Conversely, if you receive a <span class="math inline">\(1\)</span>, you know that you have to keep on reading the bit stream to decode a new symbol. So the decoding process is easy.</p>
<p>Codes that require no look-ahead and are therefore easy to decode are called <strong>prefix codes</strong>: there is no valid code, that, completed with extra bits, would form another valid code.</p>
<p>So prefix codes make streaming non-ambiguous <em>and</em> easy. However are they general enough ? The answer is yes: if the Kraft inequality is satisfied for a code length function, there is non-ambiguous code that correspond to these lengths, and <em>this code can always be selected to be prefix code</em>. We’ll prove this result later after we have studied the suitable representation of prefix codes.</p>
</section><section id="example-of-variable-length-code-unicode-and-utf-8" class="level3">
<h3><a href="#example-of-variable-length-code-unicode-and-utf-8">Example of variable-length code: Unicode and utf-8</a></h3>
<blockquote>
<p>Unicode is a computing industry standard for the consistent encoding, representation and handling of text expressed in most of the world’s writing systems. The Unicode Consortium, the nonprofit organization that coordinates Unicode’s development, has the ambitious goal of eventually replacing existing character encoding schemes with Unicode and its standard Unicode Transformation Format (UTF) schemes, as many of the existing schemes are limited in size and scope and are incompatible with multilingual environments.</p>
<p>Source: <a href="http://en.wikipedia.org/wiki/Unicode" class="uri">http://en.wikipedia.org/wiki/Unicode</a></p>
</blockquote>
<p>Unicode was developed in conjunction with the Universal Character Set standard and published in book form as The Unicode Standard. As of 2011, the latest version of Unicode is 6.0 and consists of an alphabet of 249,031 characters or graphemes among 1,114,112 possible <strong>code points</strong>. Code points are designated by the symbol <code>U+X</code> where <code>X</code> is the hexadecimal representation of an integer between <code>0</code> to <code>10ffff</code>. Unicode can be implemented by different character encodings with the most commonly used encoding being UTF-8. UTF-8 uses one byte for any ASCII characters whose code point is between U+00 and U+7f, and up to four bytes for other characters. The coding process is the following: first, if the code point is in the range <code>U+0</code> – <code>U+7f</code>, the UTF-8 code is <span class="math inline">\(0\)</span> followed by the code point binary representation. Otherwise, the code point binary representation requires more than <span class="math inline">\(7\)</span> bits. The UTF-8 code then begins with the unary representation of the number of bytes used to represent the code point given that all bits of the first byte not used by the unary coding may be used and that beyond the first byte, every byte start with <code>10</code> which leaves <span class="math inline">\(6\)</span> usable bits. That is, if the code point binary representation needs <span class="math inline">\(n\)</span> bits, the number of bytes <span class="math inline">\(N\)</span> uses by the UTF-8 encoding is <span class="math display">\[
  N = 
    1 \, \mbox{ if } \, n \leq 7, \;
    \left\lfloor \frac{n-1}{5} \right\rfloor \, \mbox{ if } \, 8 \leq n \leq 31.
  \]</span></p>
<p>The following table summarize the utf-8 code layout.</p>
<div class="table"><table>
<thead>
<tr class="header">
<th style="text-align: left;">Range</th>
<th style="text-align: left;">Code Format</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>U+0</code> – <code>U+7f</code></td>
<td style="text-align: left;"><code>0xxxxxxx</code></td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>U+80</code> – <code>U+7ff</code></td>
<td style="text-align: left;"><code>110xxxxx 10xxxxxx</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>U+800</code> – <code>U+ffff</code></td>
<td style="text-align: left;"><code>1110xxxx 10xxxxxx 10xxxxxx</code></td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>U+100000</code> – <code>U+1fffff</code></td>
<td style="text-align: left;"><code>11110xxx 10xxxxxx 10xxxxxx 10xxxxxx</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>U+200000</code> – <code>U+3ffffff</code></td>
<td style="text-align: left;"><code>111110xx 10xxxxxx 10xxxxxx 10xxxxxx</code></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><code>10xxxxxx</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><code>U+4000000</code> – <code>U+7fffffff</code></td>
<td style="text-align: left;"><code>1111110x 10xxxxxx 10xxxxxx 10xxxxxx</code></td>
</tr>
<tr class="even">
<td style="text-align: left;"></td>
<td style="text-align: left;"><code>10xxxxxx 10xxxxxx</code></td>
</tr>
</tbody>
</table></div>
<p>As an example, let’s consider the code point <code>U+2203</code> that represents the character <span class="math inline">\(\exists\)</span>. It is an element of the mathematical operators whose range is <code>U+2200</code>–<code>U+22ff</code> (see <a href="http://www.unicode.org/charts/PDF/U2200.pdf" class="uri">http://www.unicode.org/charts/PDF/U2200.pdf</a>). Being in the range <code>U+0800</code> – <code>U+ffff</code>, it needs 3 bytes to be represented in utf-8. The binary representation of <code>2203</code> – as a hexadecimal number – is</p>
<pre><code>00100010 00000011</code></pre>
<p>We should fit those <span class="math inline">\(16\)</span> bits into the <span class="math inline">\(16\)</span> <code>x</code> slots of the following pattern. No zero padding (adding zeros on the left) is necessary.</p>
<pre><code>1110xxxx 10xxxxxx 10xxxxxx</code></pre>
<p>So, we split the binary representation into</p>
<pre><code>0010 001000 000011</code></pre>
<p>and intertwin the result to obtain</p>
<pre><code>11100010 10001000 10000011</code></pre>
<p>or in hexadecimal</p>
<pre><code>e2 88 83</code></pre>
<p>This is confirmed by the output of the following Python interactive session:</p>
<pre><code>&gt;&gt;&gt; exists = u"\u2203"
&gt;&gt;&gt; print exists
∃
&gt;&gt;&gt; exists.encode("utf-8")
'\xe2\x88\x83'</code></pre>
<p>The utf-8 coding may be used in music format to store text information. But given that it really applies to integers, the code point interpretation is sometimes dropped entirely. For example, the FLAC format (<a href="http://flac.sourceforge.net/format.html" class="uri">http://flac.sourceforge.net/format.html</a>) uses a “utf-8” coding to store sample number and frame number for variable block sizes.</p>
</section><section id="prefix-codes-representation" class="level3">

<div class="p"><h3><a href="#prefix-codes-representation">Prefix Codes Representation</a></h3>Prefix codes may seem to be hard to design. But as a matter of fact suitable representations of such structures make the process quite easy: binary trees and arithmetic representations are here the key structures.</div>
<section id="binary-trees" class="level4">

<div class="p"><h4><a href="#binary-trees">Binary Trees</a></h4>Binary trees are sets of nodes organised in a hierarchical structure where each node has at most two children. Formally, a <strong>binary tree</strong> is a pair <span class="math inline">\((\mathfrak{N}, \downarrow)\)</span> where <span class="math inline">\(\mathfrak{N}\)</span> is a set of elements called <strong>nodes</strong> and <span class="math inline">\(\downarrow\)</span> is the <strong>children mapping</strong>, a partial application <span class="math display">\[
  \cdot \downarrow \cdot: \mathrm{dom} \, (\downarrow) \subset \mathfrak{N} \times \{0,1\} \to \mathfrak{N}
  \]</span> such that there exist a unique <strong>root node</strong> <span class="math inline">\(r\)</span> <span class="math display">\[
  \exists \, ! \; r \in \mathfrak{N}, \,
  \forall \, n \in \mathfrak{N},  \,
  \forall \, i \in \{0, 1\}, \,  
  r \neq n \downarrow i,
  \]</span> nodes are not shared <span class="math display">\[
  n \downarrow i  = n' \downarrow i'
  \; \implies \;
  i = i' \, \wedge \, b = b'
  \]</span> and there is no cycle <span class="math display">\[
  n_0 \downarrow i_0 = n_1, \, \cdots, \, n_{p-1} \downarrow i_{p-1} = n_p
  \; \implies \;
  (n_i = n_j \implies i=j).
  \]</span> A node <span class="math inline">\(n\)</span> is <strong>terminal</strong> (or is a <strong>leaf node</strong>) if it has no children: <span class="math display">\[
  (n,0) \not \in \mathrm{dom} \,(\downarrow)\; \mbox{ and } \; (n,1) \not \in \mathrm{dom} \,(\downarrow)
  \]</span> The <strong>depth</strong> <span class="math inline">\(|n|\)</span> of a node <span class="math inline">\(n \in \mathfrak{N}\)</span> is the unique non-negative integer <span class="math inline">\(d\)</span> defined by <span class="math display">\[
  \exists \, (b_0, \cdots, b_{|d|-1}) \in \{0,1\}^{|d|}, \;
  r \downarrow b_0 \downarrow \cdots \downarrow b_{|d|-1} = n.
  \]</span></div>
</section>
<section id="codes-are-binary-trees" class="level4">

<div class="p"><h4><a href="#codes-are-binary-trees">Codes are Binary Trees</a></h4>Let <span class="math inline">\(\mathcal{A}\)</span> be a <em>finite</em> alphabet and <span class="math inline">\(c\)</span> a binary code on <span class="math inline">\(\mathcal{A}\)</span>. Let <span class="math inline">\(\mathfrak{N}\)</span> be the subset of <span class="math inline">\(\{0,1\}^{\ast}\)</span> of all prefixes of valid codes, that is <span class="math display">\[
  \mathfrak{N} 
  = 
  \{ b \in \{0,1\}^{\ast}, \; 
     \exists \, b' \in \{0,1\}^{\ast}, \, 
     \exists \, a \in \mathcal{A}, \, 
     c(a) =  b b'
  \}
  \]</span> We define the children mapping <span class="math inline">\(\cdot \downarrow \cdot\)</span> by: <span class="math display">\[
  b_0 \cdots b_{n-1} \downarrow b_n = b_0 \cdots b_{n-1} b_n 
  \]</span> provided that <span class="math inline">\(b_0 \cdots b_{n-1}\)</span> and <span class="math inline">\(b_0 \cdots b_{n-1} b_n\)</span> both belong to <span class="math inline">\(\mathfrak{N}\)</span>.</div>
<p>With this definition, <span class="math inline">\((\mathfrak{N}, \downarrow)\)</span> is a binary tree whose root is <span class="math inline">\(\epsilon\)</span> and the depth of a node is the length of the bit sequence so that the notation used in both contexts <span class="math inline">\(|b_0\cdots b_{n-1}|\)</span> is not ambiguous. We also define a partial <strong>label mapping</strong> <span class="math inline">\(S\)</span> on <span class="math inline">\(\mathfrak{N}\)</span> with values in <span class="math inline">\(\mathcal{A}\)</span> by <span class="math display">\[
  S(b_0\cdots b_{n-1}) = a \; \mbox{ if } \; c(a) = b_0 \cdots b_{n-1}
  \]</span> <span class="math inline">\(S\)</span> is a one-to-one mapping from its domain to <span class="math inline">\(\mathcal{A}\)</span>.</p>
<p>Conversely, given a tree <span class="math inline">\((\mathfrak{N}, \downarrow)\)</span> and a one-to-one partial label mapping <span class="math inline">\(S\)</span> with values in <span class="math inline">\(\mathcal{A}\)</span>, we may define uniquely a code: let <span class="math inline">\(r\)</span> be the root node ; for any <span class="math inline">\(a \in \mathcal{A}\)</span>, there is a unique sequence <span class="math inline">\(b_0 \cdots b_{n-1}\)</span> such that <span class="math display">\[
  S(r\downarrow b_0 \downarrow b_1 \downarrow \cdots \downarrow b_{n-1}) = a,
  \]</span> so that we may set <span class="math display">\[
  c(a) = b_0\cdots b_{n-1}
  \]</span> In other words, <span class="math inline">\(c(a)\)</span> is the path from the root node to the node whose label is <span class="math inline">\(a\)</span>.</p>
<p>Binary trees used in this context are usually trimmed down so that all terminal nodes in <span class="math inline">\(\mathfrak{N}\)</span> are labelled – belong to the domain of <span class="math inline">\(S\)</span>; extra nodes are useless to define the code <span class="math inline">\(c\)</span>. We say that those trees are <strong>compact</strong> with respect to <span class="math inline">\(S\)</span> and that <span class="math inline">\((\mathfrak{N}, \downarrow, S)\)</span> is a <strong>(compact, binary, labelled) tree representation</strong> of the code <span class="math inline">\(c\)</span>. Among such trees, the ones that correspond to prefix codes are easy to spot: only their terminal nodes are labelled.</p>
</section>
<section id="arithmetic-representation" class="level4">

<div class="p"><h4><a href="#arithmetic-representation">Arithmetic representation</a></h4>Let <span class="math inline">\(b_0 \cdots b_{n-1} \in \{0,1\}^{n}\)</span>. The <strong>(binary) fraction</strong> representation – denoted <span class="math inline">\(0.b_0\cdots b_{n-1}\)</span> – of this bit sequence is <span class="math display">\[
  0.b_0\cdots b_{n-1} = \sum_{i=0}^{n-1} b_i \times 2^{-i+1}.
  \]</span> To any such bit sequence <span class="math inline">\(b\)</span> we may associate an interval in <span class="math inline">\([0,1)\)</span> by: <span class="math display">\[
  b = b_0 \cdots b_{n-1} \; \rightarrow \; I(b) = [0.b_0\cdots b_{n-1}, 0.b_0\cdots b_{n-1} + 2^{-n})
  \]</span> We may notice that <span class="math inline">\(I(b)\)</span> is the only interval that contains all binary fractions (whose denominator is a power of two) whose first bits as a binary fraction are <span class="math inline">\(b_0\)</span>, <span class="math inline">\(\cdots\)</span>, <span class="math inline">\(b_{n-1}\)</span>. In other words, it contains the fraction representation of all codes of which <span class="math inline">\(b_0 \cdots b_{n-1}\)</span> is a prefix. As a consequence, for any prefix code <span class="math inline">\(c\)</span> on <span class="math inline">\(\mathcal{A}\)</span> the intervals <span class="math inline">\(I(c(a))\)</span>, <span class="math inline">\(a\in\mathcal{A}\)</span> are non-overlapping: <span class="math display">\[
   \forall \, (a, a') \in \mathcal{A}^2, \;
   a \neq a' \, \implies I(a) \cap I(a') = \varnothing
   \]</span> This property is actually necessary and sufficient for codes to be prefix codes. We use that crucial property to proof the converse of Kraft’s theorem, namely:</div>
</section>
</section><section id="proposition--kraft-converse-theorem." class="level3">

<div class="p"><h3><a href="#proposition--kraft-converse-theorem.">Proposition – Kraft Converse Theorem.</a></h3>‌ Given an alphabet <span class="math inline">\(\mathcal{A}\)</span> and a list of code lengths <span class="math inline">\((l_a), \, a \in \mathcal{A}\)</span> such that <span class="math display">\[
  K = \sum_{a \in \mathcal{A}} 2^{-l_a} \leq 1
  \]</span> there is a prefix code <span class="math inline">\(c\)</span> such that <span class="math display">\[
  \forall \, a \in \mathcal{A}, \; |c(a)| = l_a
  \]</span></div>
</section><section id="proof.-2" class="level3">

<div class="p"><h3><a href="#proof.-2">Proof.</a></h3>‌ Let’s order all symbols in <span class="math inline">\(\mathcal{A}\)</span> in a sequence <span class="math inline">\(a_0\)</span>, <span class="math inline">\(a_1\)</span>, <span class="math inline">\(\cdots\)</span>. Given the lengths <span class="math inline">\(l_a\)</span>, <span class="math inline">\(a \in \mathcal{A}\)</span>, we define the sequence of binary fractions and intervals <span class="math inline">\(x_{a_0}=0\)</span>, <span class="math inline">\(x_{a_n} = x_{a_{n-1}} + 2^{-l_{a_{n-1}}}\)</span> and <span class="math inline">\(I(a_{n}) = [x_{a_n}, x_{a_{n+1}})\)</span>. At any step <span class="math inline">\(n\)</span> of this process, <span class="math inline">\(x_{a_n} = \sum_{i=0}^{n-1} 2^{-l_{a_i}} \leq 1\)</span> because Kraft’s inequality holds. We may therefore define the code <span class="math inline">\(c\)</span> by <span class="math display">\[
  c(a_n) = b_0\cdots b_{p-1} 
  \; \mbox{ with } \;
  I(a_n) = [0.b_0\cdots b_{p-1}, 0.b_0\cdots b_{p-1}  + 2^{-p})
  \]</span> By construction these intervals are non-overlapping and consequently what we have build is a prefix code. <span class="tombstone" style="float:right;">‌<span class="math inline">\(\blacksquare\)</span></span></div>
</section></section><section id="optimal-length-coding" class="level2">
<h2><a href="#optimal-length-coding">Optimal Length Coding</a></h2>
<p>Given an alphabet and random symbol in it, we explain in this section what bounds exist on codes average bit length and how we can build a code that is optimal for this criteria.</p>
<section id="average-bit-length-bounds" class="level3">

<div class="p"><h3><a href="#average-bit-length-bounds">Average Bit Length – Bounds</a></h3>Let <span class="math inline">\(\mathcal{A}\)</span> be an alphabet, <span class="math inline">\(A\)</span> a random symbol in <span class="math inline">\(\mathcal{A}\)</span> and <span class="math inline">\(c\)</span> a code on <span class="math inline">\(\mathcal{A}\)</span>. We define the <strong>(average) (bit) length</strong> of the code as: <span class="math display">\[
  \mathbb{E} |c(A)| = \sum_{a \in \mathcal{A}} p(a) |c(a)|
  \]</span> where as usual <span class="math inline">\(p(a) = P(A=a)\)</span>. Let <span class="math inline">\((\mathfrak{N}, \downarrow, S)\)</span> is a tree representation of <span class="math inline">\(c\)</span> and define the <strong>weight</strong> <span class="math inline">\(w(n) = p(c^{-1}(n))\)</span>. The <strong>weighted tree</strong> <span class="math inline">\(t=(\mathfrak{N}, \downarrow, S, w)\)</span> has a <strong>(weighted) depth</strong> <span class="math inline">\(|t|\)</span> given by <span class="math display">\[
  |t| = \sum_{n \in \mathrm{dom} \, S} w(n) |n|
  \]</span> that is equal to <span class="math inline">\(\mathbb{E} |c(A)|\)</span>.</div>
<p>We know from the previous section that we can restrict our search to prefix codes. For such codes, the entropy <span class="math inline">\(H(A)\)</span> provides bounds on the expected length. Specifically, we have:</p>
</section>
<section id="proposition." class="level3">

<div class="p"><h3><a href="#proposition.">Proposition.</a></h3>‌Every prefix code <span class="math inline">\(c\)</span> for <span class="math inline">\(\mathcal{A}\)</span> satisfies <span class="math display">\[
  H(A) \leq \mathbb{E}sp \, |c(A)|.
  \]</span> Moreover there exist a prefix code <span class="math inline">\(c\)</span> for <span class="math inline">\(\mathcal{A}\)</span> such that <span class="math display">\[
  \mathbb{E}\,  |c(A)|  &lt; H(A) + 1.
  \]</span></div>
The inequality () may usefully be detailled. First, to the code <span class="math inline">\(c\)</span> we have to associate a set of <strong>implicit probabilities</strong> <span class="math inline">\(q(a)\)</span>, <span class="math inline">\(a \in \mathcal{A}\)</span>, by <span class="math display">\[
  q(a) \propto 2^{-|c(a)|}, \; 
  \sum_{a \in \mathcal{A}} q(a) = 1
  \]</span> Let <span class="math inline">\(p(a) = P(A=a)\)</span>. We define the <strong>(Kullback-Leibler) divergence</strong> of the probability distributions <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> by: <span class="math display">\[
  D(p || q) 
  = 
  \sum_{a \in \mathcal{A}} p(a) \log_2 \frac{p(a)}{q(a)} \geq 0
  \]</span> with the two conventions that if there is a <span class="math inline">\(a\in \mathcal{A}\)</span> such that <span class="math inline">\(q(a) = 0\)</span> and <span class="math inline">\(p(a) \neq 0\)</span> then <span class="math inline">\(D(p || q) = +\infty\)</span> and that <span class="math inline">\(p(a) \log_2 \frac{p(a)}{q(a)} = 0\)</span> when <span class="math inline">\(p(a) = 0\)</span>. The positivity of the divergence is called the <strong>Gibbs inequality</strong>. It results from a convexity argument and moreover <span class="math display">\[
   D(p || q) =  0 
   \; \mbox{ iff } \;
   \forall \, a  \in \mathcal{A}, \, p(a) = q(a)
   \]</span> These terms being given, we have the equality (proved below)
<span class="math display">\[\begin{equation}
  H(a) = \mathbb{E} |c(A)| - D(p || q) - \log_2 K(c)
  \end{equation}\]</span>
where <span class="math inline">\(K(c) \leq 1\)</span> is defined in the Kraft inequality (). The equality case in the inequality () happens if and only if
<span class="math display">\[\begin{equation}
  \forall \, a \in \mathcal{A}, |c(a)| = -\log_2 p(a)
  \end{equation}\]</span>
a condition for which we may find a code <span class="math inline">\(c\)</span> if and only if the probability <span class="math inline">\(p\)</span> satisfies
<span class="math display">\[\begin{equation}
  \forall \, a \in \mathcal{A}, \;
  p(a) \in 2^{-\mathbb{N}}
  \end{equation}\]</span>
</section>
<section id="proof.-3" class="level3">
<h3><a href="#proof.-3">Proof.</a></h3>
‌ The mean code length satisfies <span class="math display">\[
  \mathbb{E} |c(A)| = \sum_{a \in \mathcal{A}} p(a) |c(a)|
  = - \sum_{a \in \mathcal{A}} p(a) \log_2 2^{-|c(a)|}
  \]</span> Given the definition of the implicit probability <span class="math display">\[
  q(a) = \frac{2^{-|c(a)|}}{\sum_{a\in\mathcal{A}} 2^{-|c(a)|}} = \frac{2^{-|c(a)|}}{K(c)},
  \]</span> we end up with
<span class="math display">\[\begin{eqnarray*}
  \mathbb{E} |c(A)| &amp;=&amp; - \sum_{a \in \mathcal{A}} p(a) \log_2 q(a) - \log_2 K(c) \\
                    &amp;=&amp; - \sum_{a \in \mathcal{A}} p(a) \log_2 p(a) - \sum_{a \in \mathcal{A}} p(a) \log_2 \frac{q(a)}{p(a)} - \log_2 K(c) \\
                    &amp;=&amp; H(A)  + D(p || q) - \log_2 K(c) \\ &amp;\geq&amp; H(A) \\ 
  \end{eqnarray*}\]</span>
<p>For any <span class="math inline">\(a\in \mathcal{A}\)</span>, we define <span class="math inline">\(l(a) = \lceil - \log_2 p(a) \rceil\)</span>. As we have <span class="math display">\[
  \sum_{a \in \mathcal{A}} 2^{-l(a)}
  \leq
  \sum_{a \in \mathcal{A}} 2^{\log_2 p(a)}
   = \sum_{a \in \mathcal{A}} p(a) = 1
  \]</span> there is a prefix code <span class="math inline">\(c\)</span> that satisfies <span class="math inline">\(|c(a)| = l(a)\)</span> for any <span class="math inline">\(a \in \mathcal{A}\)</span>. Moreover <span class="math display">\[
  \mathbb{E} |c(A)| = \sum_{a \in \mathcal{A}} p(a) l(a)
  &lt;
  \sum_{a \in \mathcal{A}} p(a) (- \log_2 p(a) + 1)
  = H(A) + 1.
  \]</span> <span class="tombstone" style="float:right;">‌<span class="math inline">\(\blacksquare\)</span></span></p>
</section>
<section id="algorithm-and-implementation-of-huffman-coding" class="level3">

<div class="p"><h3><a href="#algorithm-and-implementation-of-huffman-coding">Algorithm and Implementation of Huffman Coding</a></h3>We represent (finite) alphabets and the probability distribution of a random symbol as a single Python dictionary, for example <span class="math display">\[
   \mathcal{A} = \{\mbox{`$a$'}, \mbox{`$b$'}, \mbox{`$c$'}\} \; \mbox{ and } \;
   p(\mbox{`$a$'}) = 0.5, \; p(\mbox{`$b$'}) = 0.3, \; p(\mbox{`$c$'}) = 0.2 
   \]</span> as {‘a’: 0.5, ‘b’: 0.3, ‘c’: 0.2}</div>
<p>Probabilities will be used as relative <strong>weights</strong>: instead we could define the dictionary as</p>
<pre><code>{'a': 5, 'b': 3, 'c': 2}</code></pre>
<p>and the code resulting from the Huffman algorithm would be the same.</p>
<p>The algorithm we implement uses <strong>weighted binary trees</strong> where:</p>
<ul>
<li><p>terminal nodes are symbol/weight pairs such as</p>
<pre><code>('a', 1.0)</code></pre></li>
<li><p>non-terminal nodes are children/weight pairs where children is a list of (terminal or non-terminal) nodes, such as</p>
<pre><code>([('c': 0.2), ('a': 0.5)], 0.7)</code></pre></li>
</ul>
<p>Handling of these structures is made through a small set of helper functions:</p>
<pre><code>class Node(object):
    "Function helpers to manage nodes as (symbol, weight) pairs"

    @staticmethod
    def symbol(node):
        return node[0]

    @staticmethod
    def weight(node):
        return node[1]

    @staticmethod
    def is_terminal(node):
        return not isinstance(Node.symbol(node), list)</code></pre>
<p>The <strong>Huffman algorithm</strong> is the following: we create a list of symbol/weight nodes from the initial dictionary and repeat the following steps:</p>
<ol type="1">
<li><p>pick up two nodes with the least weights, remove them from the list,</p></li>
<li><p>insert into the list a non-terminal nodes with those two nodes as children and a weight that is the sum of their weights,,</p></li>
<li><p>stop when there is a single node in the list: this is the root of the binary tree</p></li>
</ol>
<p>Note that this algorithm is not entirely deterministic if at any step, the lowest weight correspond to three nodes or more.</p>
<p>Here is the corresponding implementation in <code>make_binary_tree</code>; note that in the other methods of the <code>huffman</code> class, we also create a symbol to code dictionary (<code>self.table</code>) from the binary tree to simplify the coding and decoding of symbols – not all functions are given in this chunk of code ; the <code>symbol_encoder</code> and <code>symbol_decoder</code> create (symbol) coders and decoders from the symbol to code tables and <code>stream_encoder</code> and <code>stream_decoder</code> create stream coder and encoder from those.</p>
<pre><code>class huffman(object):
    def __init__(self, weighted_alphabet):
        self.weighted_alphabet = weighted_alphabet
        self.tree = huffman.make_binary_tree(weighted_alphabet)
        self.table = huffman.make_table(self.tree)
        self.encoder = stream_encoder(symbol_encoder(self.table))
        self.decoder = stream_decoder(symbol_decoder(self.table))

    @staticmethod
    def make_binary_tree(weighted_alphabet):
        nodes = weighted_alphabet.items()
        while len(nodes) &gt; 1:
            nodes.sort(key=Node.weight)
            node1, node2 = nodes.pop(0), nodes.pop(0)
            node = ([node1, node2], Node.weight(node1) + Node.weight(node2))
            nodes.insert(0, node)
        return nodes[0]

    @staticmethod
    def make_table(root, table=None, prefix=None):
        if prefix is None:
            prefix = BitStream()
        if table is None:
            table = {}
        if not Node.is_terminal(root):
            for index in (0, 1):
                new_prefix = prefix.copy()
                new_prefix.write(bool(index))
                new_root = Node.symbol(root)[index]
                huffman.make_table(new_root, table, new_prefix)
        else:
            table[Node.symbol(root)] = prefix
        return table</code></pre>
<p>As usual, the coder and decoder are registered for use with the <code>BitStream</code> instances.</p>
<pre><code>bitstream.register(huffman, reader=lambda h: h.decoder, writer=lambda h: h.encoder)</code></pre>
</section>
<section id="optimality-of-the-huffman-algorithm" class="level3">

<div class="p"><h3><a href="#optimality-of-the-huffman-algorithm">Optimality of the Huffman algorithm</a></h3>We prove in this section that the Huffman algorithm creates a code whose average bit length is minimal among all prefix codes (and therefore all non-ambiguous stream codes).</div>
</section>
<section id="switching-nodes." class="level3">

<div class="p"><h3><a href="#switching-nodes.">Switching nodes.</a></h3>‌Consider a weighted tree <span class="math inline">\(t=(\mathfrak{N}, \downarrow, S, w)\)</span> and two of its terminal nodes <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>. Switch the position of these nodes and call the resulting tree <span class="math inline">\(t'\)</span>. A simple computation shows that the weighted bit length of <span class="math inline">\(t'\)</span> is given by <span class="math display">\[
  |t'| = |t| + (|n_1|_t - |n_2|_{t})(w(n_2) - w(n_1)) 
  \]</span> Consequently, a tree that has a terminal node with a high probability at a depth larger than another node with low probability higher in the tree can’t be optimal, because switching the two nodes would decrease the code average bit length. The same formula also shows that given two terminal nodes <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> among the ones with the lowest probability, we can always find an optimal tree with them as siblings at the greatest depth: at least one optimal tree exist for combinatorial reasons ; for such a tree, either a node is a leaf, or it has two children nodes; now get at the bottom of the tree: there are two terminal nodes. If they are not the desired nodes, switches that are neutral to the average bit length will create a new optimal tree with the desired property.</div>
</section>
<section id="length-optimality." class="level3">

<div class="p"><h3><a href="#length-optimality.">Length optimality.</a></h3>‌The proof that the Huffman coding algorithm is optimal is made by induction on the number of symbols. The check for the basis assumption is straightforward. Now assume that the result holds for <span class="math inline">\(N\)</span> symbols and pick an alphabet with <span class="math inline">\(N+1\)</span> symbols. Let <span class="math inline">\(t\)</span> be the tree that results from the application of the Huffman algorithm. Now consider the initial set of symbols and replace the two with the lowest weight (nodes <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>) with a single one with a weight equal to the sum of the original symbol weights. Let <span class="math inline">\(t'\)</span> be the result of the Huffman algorithm on these <span class="math inline">\(N\)</span> symbols. The average bit length of both trees are related by <span class="math inline">\(|t'| = |t| + (w(n_1) + w(n_2)) (|n_1|_t - 1) - w(n_1) |n_1|_t - w(n_2) |n_2|_t = |t| - w(n_1) - w(n_2)\)</span>. Now consider an optimal rearrangement of <span class="math inline">\(t_{\star}\)</span> of <span class="math inline">\(t\)</span> that would have <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> as siblings at the lowest level (possible by the previous section). Grouping <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span> leads to a <span class="math inline">\(t'_{\star}\)</span> whose length is <span class="math inline">\(|t'_{\star}| = |t_{\star}| - w(n_1) - w(n_2)\)</span> and such <span class="math inline">\(t'_{\star}\)</span> is a rearrangement of <span class="math inline">\(t'\)</span>. Hence, as <span class="math inline">\(|t| = |t_{\star}| - (|t'_{\star}| - |t'|)\)</span> and because by the induction hypothesis <span class="math inline">\(t'\)</span> is optimal, <span class="math inline">\(|t'_{\star}| - |t'| \geq 0\)</span>, we have <span class="math inline">\(|t| \leq |t^{\star}|\)</span> and <span class="math inline">\(t\)</span> is optimal.</div>
</section>
<section id="example-brainfuck-spoon-and-fork" class="level3">

<div class="p"><h3><a href="#example-brainfuck-spoon-and-fork">Example: Brainfuck, Spoon and Fork</a></h3><a href="http://www.muppetlabs.com/~breadbox/bf/">Brainfuck</a> is an eight-instruction Turing-complete programming language. The eight instructions are represented by the characters:</div>
<pre><code>&gt; \;\; &lt; \;\; + \;\; - \;\; . \;\; , \;\; [ \;\; ]</code></pre>
<p>The language is organised around a single byte pointer <code>p</code>. As an exemple, the instruction “+<code>' corresponds to the C code fragment  "</code>(*p)++;<code>"​ and ".</code>’ to “<code>putchar(*p);</code>”​. The classic “Hello World!” program in Brainfuck is the sequence:</p>
<pre><code>++++++++++[&gt;+++++++&gt;++++++++++&gt;+++&gt;+&lt;&lt;&lt;&lt;-]
&gt;++.&gt;+.+++++++..+++.&gt;++.&lt;&lt;+++++++++++++++.
&gt;.+++.------.--------.&gt;+.&gt;.</code></pre>
<p><a href="http://web.archive.org/web/20100226062821/http://progopedia.com/dialect/spoon/">Spoon</a> is a variant of Brainfuck created in 1998 by Steven Goodwin. It uses binary encoding of the eight original symbols according to the mapping</p>
<div class="table"><table>
<colgroup>
<col style="width: 24%">
<col style="width: 27%">
<col style="width: 24%">
<col style="width: 24%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>&gt;</code> <span class="math inline">\(\rightarrow\)</span> 010</td>
<td style="text-align: left;"><code>&lt;</code> <span class="math inline">\(\rightarrow\)</span> 011</td>
<td style="text-align: left;"><code>+</code> <span class="math inline">\(\rightarrow\)</span> 1</td>
<td style="text-align: left;"><code>-</code> <span class="math inline">\(\rightarrow\)</span> 000</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>.</code> <span class="math inline">\(\rightarrow\)</span> 001010</td>
<td style="text-align: left;"><code>,</code> <span class="math inline">\(\rightarrow\)</span> 0010110</td>
<td style="text-align: left;"><code>[</code> <span class="math inline">\(\rightarrow\)</span> 00100</td>
<td style="text-align: left;"><code>]</code> <span class="math inline">\(\rightarrow\)</span> 0011</td>
</tr>
</tbody>
</table></div>
<p>The code is a prefix code that was actually designed using Huffman encoding, with probabilities of each command computed from a set of example programs. You may notice that the code appears NOT to be optimal as <code>,</code> has the longest code and has no sibling: it could be shortened to <span class="math inline">\(001011\)</span>. But that’s only because the <em>actual</em> Spoon variant of Brainfuck introduces two extra reserved codes, <span class="math inline">\(00101110\)</span> to dump the memory (for debug purposes) and <span class="math inline">\(00101111\)</span> to stop execution. The full binary tree of the code – where the corresponding symbols are <code>*</code> and <code>!</code> – and the arithmetic representation are displayed in figure .</p>
<figure>
<img src="images/brainfuck-tree.svg" alt="Spoon Coding Tree"><figcaption>Spoon Coding Tree</figcaption>
</figure>
<figure>
<img src="images/brainfuck_arithmetic.svg" alt="Spoon Coding Arithmetic Representation"><figcaption>Spoon Coding Arithmetic Representation</figcaption>
</figure>
<p>The Spoon encoding of the “Hello World!” program is a sequence of 245 bits:</p>
<pre><code>11111111110010001011111110101111111111010111010101
10110110110000011010110010100101001010111111100101
00010101110010100101100101001101111111111111111100
10100100010101110010100000000000000000000010100000
000000000000000000000010100101001010010001010</code></pre>
<p>As the commands are exactly <span class="math inline">\(8\)</span>, it is also tempting to define a 3-bit fixed-length encoding of the original symbols and let’s call it Fork ! We adopt the mapping:</p>
<div class="table"><table>
<colgroup>
<col style="width: 24%">
<col style="width: 27%">
<col style="width: 24%">
<col style="width: 24%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;"><code>&gt;</code> <span class="math inline">\(\rightarrow\)</span> 000</td>
<td style="text-align: left;"><code>&lt;</code> <span class="math inline">\(\rightarrow\)</span> 001</td>
<td style="text-align: left;"><code>+</code> <span class="math inline">\(\rightarrow\)</span> 010</td>
<td style="text-align: left;"><code>-</code> <span class="math inline">\(\rightarrow\)</span> 011</td>
</tr>
<tr class="even">
<td style="text-align: left;"><code>.</code> <span class="math inline">\(\rightarrow\)</span> 100</td>
<td style="text-align: left;"><code>,</code> <span class="math inline">\(\rightarrow\)</span> 101</td>
<td style="text-align: left;"><code>[</code> <span class="math inline">\(\rightarrow\)</span> 110</td>
<td style="text-align: left;"><code>]</code> <span class="math inline">\(\rightarrow\)</span> 111</td>
</tr>
</tbody>
</table></div>
<p>The encoding with Fork of the “Hello World!” program is</p>
<pre><code>01001001001001001001001001001011000001001001001001
00100100000100100100100100100100100100100000100100
10000010001001001001011111000010010100000010100010
01001001001001001010010001001001010000001001010000
10010100100100100100100100100100100100100100100101
00000100010010010100011011011011011011100011011011
011011011011011100000010100000100</code></pre>
<p>a stream of 333 binary digits, about 36 % less space-efficient than spoon.</p>
</section>
</section><section id="golomb-rice-coding" class="level2">
<h2><a href="#golomb-rice-coding">Golomb-Rice Coding</a></h2>
<section id="optimality-of-unary-coding-reduced-sources" class="level3">

<div class="p"><h3><a href="#optimality-of-unary-coding-reduced-sources">Optimality of Unary Coding – Reduced Sources</a></h3>Consider the alphabet <span class="math inline">\(\mathbb{N}\)</span> of the non-negative integers and a random symbol <span class="math inline">\(n\)</span> in <span class="math inline">\(\mathbb{N}\)</span> such that for any integer <span class="math inline">\(i\)</span> <span class="math display">\[
   P(n=i) = P(n&gt;i)
   \]</span> Normalization of this probability distribution <span class="math inline">\(p(i) = P(n=i)\)</span> yields <span class="math display">\[
   \forall \, i \in \mathbb{N}, \, p(i) = 2^{-i-1}
   \]</span> What is the optimal coding of such a random symbol ? An infinite number of symbols have a non-zero probability, therefore we can’t apply directly Huffman coding. However, we can get a flavor of what the optimal coding can be by reducing the random symbol to a finite alphabet. To do so, we select a threshold <span class="math inline">\(m\)</span> and group together all the outcomes of <span class="math inline">\(n\)</span> greater or equal to <span class="math inline">\(m\)</span>; the new random symbol based on <span class="math inline">\(n\)</span> has values in the finite alphabet whose symbols are <span class="math inline">\(0\)</span>, <span class="math inline">\(1\)</span>, <span class="math inline">\(2\)</span>, <span class="math inline">\(\cdots\)</span>, <span class="math inline">\(m-1\)</span> and the set <span class="math inline">\(\{m,m+1,\cdots\}\)</span> of values above <span class="math inline">\(m\)</span>. The probability of the <span class="math inline">\(m\)</span> first <span class="math inline">\(m\)</span> symbols <span class="math inline">\(i\)</span> is <span class="math inline">\(p_m(i) = P(n=i) = 2^{-i-1}\)</span> and the probability of the last one is <span class="math display">\[
  p_m(i) = P(n \geq m) = \sum_{i=m}^{+\infty} P(n=i) 
  = \sum_{i=m}^{+\infty} 2^{-i-1}
  = 2^{-m}
  \]</span> Let’s apply the Huffman algorithm to the resulting symbol, say for <span class="math inline">\(m=2\)</span>. Here are the steps that build the Huffman binary tree: the initial list of symbols, sorted by increasing probability is:</div>
<figure>
<img src="images/huffmann-1.svg">
</figure>
<p>The nodes <span class="math inline">\(\{3,4,\cdots\}\)</span> and <span class="math inline">\(2\)</span> have the lowest probability of occurence and therefore shall be grouped. But their probability is the same – <span class="math inline">\(0.125\)</span> – and we need to make the a decision about which one will be taken as the first node of the group. We decide to go for the infinite set of symbols <span class="math inline">\(\{3,4,\cdots\}\)</span> first. Somehow it feels the right thing to do, to have the only compound symbol on the left and all the others on the right …</p>
<figure>
<img src="images/huffmann-2.svg">
</figure>
<p>This group has a cumulative probability of <span class="math inline">\(0.25\)</span>, the same as the symbol <span class="math inline">\(1\)</span>. There is only one extra symbol, <span class="math inline">\(0\)</span> and its probability is higher, therefore no reordering is necessary. We therefore group <span class="math inline">\(\{2,3,\cdots\}\)</span> (first) and <span class="math inline">\(1\)</span>.</p>
<figure>
<img src="images/huffmann-3.svg">
</figure>
<p>There are only two symbols left, so the algorithm has completed. Let’s look at the results in terms of coding: we ended up with the code <span class="math display">\[
0 \rightarrow 1, \; 1 \rightarrow 01, \; 2 \rightarrow 001, \{3,4,\cdots\} \rightarrow 000
\]</span> From this we can guess what the optimal coding is for the original source, without the reduction attached to the threshold : it is the unary coding of the integer, or more precisely here, the variant that uses <span class="math inline">\(0\)</span> for the length and <span class="math inline">\(1\)</span> as an end symbol. <span class="math display">\[
   0 \rightarrow 1, \; 1 \rightarrow 01, \; 2 \rightarrow 001, 
\; 3 \rightarrow 0001, \; 4 \rightarrow 00001, \; 5 \rightarrow 000001, \; \cdots
\]</span></p>
<p>The choice of the variant has no impact on the optimality of the code: unary coding is the optimal solution for the coding of non-negative integers that occur with a probability <span class="math inline">\(p(i) = 2^{-i-1}\)</span>. Here is an implementation of unary coding of integer symbols that uses <span class="math inline">\(0\)</span> as an end delimiter instead:</p>
<pre><code>def unary_symbol_encoder(stream, symbol):
    return stream.write(symbol * [True] + [False], bool)

def unary_symbol_decoder(stream):
    count = 0
    while stream.read(bool) is True:
        count += 1
    return count</code></pre>
<p>Given those symbol encoder and decoder function, the higher-order function <code>stream_encoder</code> and <code>stream_decoder</code> from the <code>coding</code> module generate a stream encoder and decoder respectively.</p>
<pre><code>unary_encoder = stream_encoder(unary_symbol_encoder)
unary_decoder = stream_decoder(unary_symbol_decoder)</code></pre>
<p>Finally, we define an empty class <code>unary</code> as a type used to register the encoder and decoder in the <code>bitstream</code> module.</p>
<pre><code>class unary(object):
    pass
bitstream.register(unary, reader=unary_decoder, writer=unary_encoder)</code></pre>
<p>After that last step, using unary coding and decoding is as simple as:</p>
<pre><code>&gt;&gt;&gt; stream = BitStream()
&gt;&gt;&gt; stream.write([0,1,2,3], unary)
&gt;&gt;&gt; print stream
0101101110
&gt;&gt;&gt; stream.read(unary, 4)
[0, 1, 2, 3]</code></pre>
</section>
<section id="optimal-coding-of-geometric-distribution---rice-coding" class="level3">

<div class="p"><h3><a href="#optimal-coding-of-geometric-distribution---rice-coding">Optimal Coding of Geometric Distribution - Rice Coding</a></h3>The same method of source reduction may be applied to analyze optimal code for more complex distribution, for example the <strong>(one-sided) geometric distribution</strong>. Such a distribution is defined by <span class="math inline">\(p(i) \propto \theta^{i}\)</span> for a <span class="math inline">\(\theta \in (0,1)\)</span>; note that the distribution of the previous section was the special case of <span class="math inline">\(\theta=0.5\)</span>. The normalization of this distribution leads to <span class="math display">\[
  p(i) = (1-\theta) \theta^i
  \]</span> The method of reduced source illustrated in the previous section may be used to derive an optimal coding in the general case. Let’s summarize the findings of  ; first, let <span class="math inline">\(l\)</span> be the unique integer such that <span class="math display">\[
    \theta^l + \theta^{l+1} \leq 1 &lt; \theta^{l-1} + \theta^l.
    \]</span> The optimal coding of the non-negative integer <span class="math inline">\(i\)</span> is made of two codes:</div>
<ul>
<li>the unary coding of <span class="math inline">\(\lfloor i/l \rfloor\)</span>, -the Huffman coding of <span class="math inline">\(i \mbox{ mod } l\)</span>.</li>
</ul>
<p>The probability distribution needed to perform the second part of the encoding is: <span class="math display">\[
  P(i \mbox{ mod } l) = \frac{(1-\theta)\theta^i}{1-\theta^l}.
  \]</span> Note that this distribution is quite flat with respect to the original one. For that reason, the length of the optimal coding are almost constant for the Huffman part. Precisely, the length of the coding of <span class="math inline">\(0 \leq i &lt; l\)</span> is <span class="math inline">\(\lfloor \log_2 l \rfloor\)</span> (if <span class="math inline">\(i&lt; 2^{\lfloor \log_2 l + 1\rfloor} - l\)</span>) or <span class="math inline">\(\lfloor \log_2 l \rfloor +1\)</span> (otherwise). In particular, if <span class="math inline">\(l\)</span> is a power of two, we have <span class="math inline">\(2^{\lfloor \log_2 l + 1\rfloor} - l = l\)</span> is therefore every <span class="math inline">\(0 \leq i &lt; l\)</span> is coded with the same length of <span class="math inline">\(\log_2 l\)</span>.</p>
</section>
<section id="rice-coding" class="level3">
<h3><a href="#rice-coding">Rice Coding</a></h3>
<section id="rice-coding-or-golomb-power-of-two-gpo2-coding" class="level4">

<div class="p"><h4><a href="#rice-coding-or-golomb-power-of-two-gpo2-coding">Rice coding (or **Golomb-Power-Of-Two (GPO2) coding)</a></h4>It uses the remark above to simplify the coding at the price of a usually negligible suboptimality: instead of the “true” integer <span class="math inline">\(l\)</span>, solution of (), we select an approximation of it <span class="math inline">\(l'=2^n\)</span> that is a power of two, then perform the coding using this value instead of <span class="math inline">\(l\)</span>. As a consequence, the Huffman coding of the second part is replaced by a fixed-length encoding of an integer on <span class="math inline">\(n\)</span> bits.</div>
<p>The remaining issue is to determine a good selection of the <strong>Golomb parameter</strong> <span class="math inline">\(n\)</span> ; this issue is described in details in . Initially, we need an estimate of <span class="math inline">\(\theta\)</span> – that is a priori unknown – from experimental data; we can usually derive it from the mean <span class="math inline">\(m\)</span> f the available values : as the expectation of a one-sided geometric random variable with parameter <span class="math inline">\(\theta\)</span> has an expectation of <span class="math inline">\(\theta/(1-\theta)\)</span>, it makes sense to select <span class="math display">\[
  \theta = \frac{m}{1+m}
  \]</span> Given the golden ratio <span class="math display">\[
  \phi = \frac{1 +\sqrt{5}}{2},
  \]</span> we select as the number of bits dedicated to the fixed-length coding the value: <span class="math display">\[
  n = \max \left[0, 1 + \left\lfloor \log_2 \left( \frac{\log (\phi -1)}{\log \theta} \right) \right\rfloor \right].
  \]</span></p>
</section>
</section>
<section id="implementation" class="level3">

<div class="p"><h3><a href="#implementation">Implementation</a></h3>We begin with the definition of a <code>rice</code> class that holds the parameters of a given Rice encoding and also provide a method for the selection of the optimal parameter. The use of this method is optional: given that Rice coding is very often applied to distributions of integers that are not geometric, there is no guarantee in general that the Golomb parameter of this method will be the most efficient selection.</div>
<p>The parameter <code>n</code> in the <code>rice</code> constructor is the Golomb parameter. The optional <code>signed</code> option may be used to enable the coding of negative integers.</p>
<pre><code>class rice(object):
    def __init__(self, n, signed=False):
        self.n = n
        self.signed = signed

    @staticmethod
    def select_parameter(mean):
        golden_ratio = 0.5 * (1.0 + numpy.sqrt(5))
        theta = mean / (mean + 1.0)
        log_ratio = log(golden_ratio - 1.0) / log(theta))
        return int(maximum(0, 1 + floor(log2(log_ratio))))</code></pre>
<p>The following encoder and decoder implementations demonstrate how we deal with signed values: by prefixing the code stream with a bit sign (0 for <span class="math inline">\(+\)</span>, 1 for <span class="math inline">\(-\)</span>) before we encode the absolute value of the integer. More complex schemes – that intertwin negative and positive values – are possible and may be useful to deal with two-sided geometric distribution that are not centered around <span class="math inline">\(0\)</span> (see for example ). We then encode the fixed-length part of the code and follow with the unary code.</p>
<p>In the following code, <code>options</code> is meant to be an instance of the <code>rice</code> class, <code>stream</code> is an instance of <code>BitStream</code> and <code>symbol</code> an integer.</p>
<pre><code>def rice_symbol_encoder(options):
    def encoder(stream, symbol):
        if options.signed:
            stream.write(symbol &lt; 0)
        symbol = abs(symbol)
        remain, fixed = divmod(symbol, 2 ** options.n)
        fixed_bits = []
        for _ in range(options.n):
            fixed_bits.insert(0, bool(fixed % 2))
            fixed = fixed &gt;&gt; 1
        stream.write(fixed_bits)
        stream.write(remain, unary)
    return encoder


def rice_symbol_decoder(options):
    def decoder(stream):
        if options.signed and stream.read(bool):
            sign = -1
        else:
            sign = 1
        fixed_number = 0
        for _ in range(options.n):
            fixed_number = (fixed_number &lt;&lt; 1) + int(stream.read(bool))
        remain_number = 2 ** options.n * stream.read(unary)
        return sign * (fixed_number + remain_number)
    return decoder</code></pre>
<p>Finally, the <code>rice</code> class, its encoder and decoder are registered for integration with the <code>BitStream</code> instances.</p>
<pre><code>rice_encoder = lambda r: stream_encoder(rice_symbol_encoder(r))
rice_decoder = lambda r: stream_decoder(rice_symbol_decoder(r))
bitstream.register(rice, reader=rice_decoder, writer=rice_encoder)</code></pre>
<p>The following Python session demonstrates the basic usage:</p>
<pre><code>&gt;&gt;&gt; data = [0, 8, 0, 8, 16, 0, 32, 0, 16, 8, 0, 8]
&gt;&gt;&gt; rice.select_parameter(mean(data))
3
&gt;&gt;&gt; stream = BitStream()
&gt;&gt;&gt; stream.write(data, rice(3))
&gt;&gt;&gt; stream
000000010000000010000110000000011110000000011000010000000010
&gt;&gt;&gt; stream.read(rice(3), 12)
[0, 8, 0, 8, 16, 0, 32, 0, 16, 8, 0, 8]</code></pre>
<p>On this particular data, the selection of the Golomb parameter was effective if we consider the following test of possible parameter values between <span class="math inline">\(0\)</span> (unary coding) and <span class="math inline">\(6\)</span>.</p>
<pre><code>&gt;&gt;&gt; for i in range(7):
...     stream = BitStream(data, rice(i))
...     print "rice n={0}: {1} bits".format(i, len(stream)) 
... 
rice n=0: 108 bits
rice n=1: 72 bits
rice n=2: 60 bits
rice n=3: 60 bits
rice n=4: 64 bits
rice n=5: 73 bits
rice n=6: 84 bits</code></pre>
</section>
</section></section>
<section class="footnotes" id="notes"><h1><a href="#notes">Notes</a></h1>
<hr>
<ol>
<li id="fn1"><p><a href="http://en.wikipedia.org/wiki/Word_(computer_architecture)" class="uri">http://en.wikipedia.org/wiki/Word_(computer_architecture)</a>.<a href="#fnref1">↩</a></p></li>
<li id="fn2"><p>see for example <a href="http://en.wikipedia.org/wiki/Two's_complement" class="uri">http://en.wikipedia.org/wiki/Two's_complement</a>.<a href="#fnref2">↩</a></p></li>
</ol>
</section>
</main>


</body></html>
